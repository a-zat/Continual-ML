{"cells":[{"cell_type":"markdown","metadata":{"id":"D8g3_OkUNOuD"},"source":["## **Import the TensorFlow library**"]},{"cell_type":"markdown","metadata":{"id":"-tkdWKKZZlAL"},"source":["Bisogna togliere le librerie che non servono o sono duplicate rispetto a quelle che già ci sono nel file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCqcQuaBLNgF"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Dropout, Dense, Flatten, Reshape\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import optimizers\n","\n","import matplotlib.pyplot as plt \n","import matplotlib.image as mpimg\n","from PIL import Image\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import random \n","import csv \n","import sys\n","import os\n","import re\n","from random import seed\n","\n","import time\n","import cv2\n","import glob\n","from keras import applications\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","from numpy.ma.core import size\n","\n","import torchvision.models as models\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{"id":"XCdpahT2WeYV"},"source":["# LOAD TRAINED MODEL AND FEATURES"]},{"cell_type":"markdown","metadata":{"id":"uDC7p2h5Y9pe"},"source":["Qui vanno chiaramente messi i nuovi path dove sono messi i txt di features, le loro labels e il file h5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBDHiZjrWeYW"},"outputs":[],"source":["MODEL_PATH = ROOT_PATH + \"/Saved_models/Backup_models/Last_trained_model/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukq10Qbjvikl"},"outputs":[],"source":["features = np.loadtxt(MODEL_PATH + 'Original_model/ll_features_10.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZgRJPXFP4V8"},"outputs":[],"source":["labels_features = np.loadtxt(MODEL_PATH + 'Original_model/ll_labels_features_10.txt')\n","labels_features = labels_features.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4XVO9QXWeYW"},"outputs":[],"source":["model = keras.models.load_model(MODEL_PATH + 'Original_model/mnist_cnn.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puX6mAw_IbXs"},"outputs":[],"source":["model_frozen = keras.models.load_model(MODEL_PATH + 'Frozen_model/omv_mnist_cnn.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIYidkyNWeYX"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"VVfY-k_6IgCZ"},"source":["# K-MEAN FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23UkPcc8pXee"},"outputs":[],"source":["def create_k_mean(data, number_of_clusters):\n","    # n_jobs is set to -1 to use all available CPU cores. This makes a big difference on an 8-core CPU\n","    # especially when the data size gets much bigger. #perfMatters\n","\n","    k = KMeans(n_clusters=number_of_clusters, n_init=100)\n","    # k = KMeans(n_clusters=number_of_clusters, n_init=20, max_iter=500)\n","\n","    # Let's do some timings to see how long it takes to train.\n","    start = time.time()\n","\n","    # Train it up\n","    k.fit(data)\n","\n","    # Stop the timing\n","    end = time.time()\n","\n","    # And see how long that took\n","    # print(\"Training took {} seconds\".format(end - start))\n","\n","    return k"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qs1R0ufTpf7e"},"outputs":[],"source":["def cluster_label_count(clusters, labels):\n","    count = {}\n","\n","    # Get unique clusters and labels\n","    unique_clusters = list(set(clusters))\n","    unique_labels = list(set(labels))\n","\n","    # Create counter for each cluster/label combination and set it to 0\n","    for cluster in unique_clusters:\n","        count[cluster] = {}\n","\n","        for label in unique_labels:\n","            count[cluster][label] = 0\n","\n","    # Let's count\n","    for i in range(len(clusters)):\n","        count[clusters[i]][labels[i]] += 1\n","\n","    cluster_df = pd.DataFrame(count)\n","\n","    return cluster_df, count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"haWEOGNQpi46"},"outputs":[],"source":["def k_mean_clustering(features_x_test, features_init, y_test, labels_features_init, n_cluster, batch_size):\n","\n","  # creo due vettori vuoti uno per le label lungo (n_features) l'altro per le features lungo (n_features,512)\n","  features = np.zeros((int(n_cluster*len(labels_features_init)/10),512))\n","  labels_features = np.zeros(int(n_cluster*len(labels_features_init)/10))\n","  k = 0 #inizializzo un counter\n","\n","  #passo il vettore salvato nel file e mi tiro fuori solo i numeri che mi interessano. Se ho 6 cluster, mi interessano solo gli 0, 1, 2, 3, 4 e 5.\n","  for i in range(0,len(features_init)-1):\n","      if labels_features_init[i] < n_cluster:\n","        features[k,:] = features_init[i,:]\n","        labels_features[k] = labels_features_init[i]\n","        k += 1\n","  \n","  labels_features = labels_features.astype(int)\n","\n","  # aggiungo la feature che di cui ci interessa trovare la pseudo label\n","  features = np.concatenate((features, features_x_test))\n","  labels_features = np.append(labels_features, y_test)\n","  n_count = np.zeros((batch_size, n_cluster)).astype(int) #inizializzo un vettore che ci permette di contare per ogni cluster quali numeri sono stati inseriti\n","\n","  # viene fatto il clustering\n","  k_mean = create_k_mean(features ,n_cluster)\n","\n","  # qui viene riempito il vettore conteggio. Se l'immagine che ci interessa classificare è stata inserita nel cluster 0, il ciclo guarderà \n","  # tutte le labels delle immagini che sono state messe in quel cluster e la label che compare più volte sarà la pseudo_label della nostra immagine\n","  for j in range(1, batch_size+1):\n","    for i in range(0,len(k_mean.labels_) - batch_size):\n","        if k_mean.labels_[i] == k_mean.labels_[-j]:\n","         if labels_features[i] == 0:\n","           n_count[-j,0] +=1\n","         if labels_features[i] == 1:\n","           n_count[-j,1] +=1\n","         if labels_features[i] == 2:\n","           n_count[-j,2] +=1\n","         if labels_features[i] == 3:\n","           n_count[-j,3] +=1\n","         if labels_features[i] == 4:\n","           n_count[-j,4] +=1\n","         if labels_features[i] == 5:\n","           n_count[-j,5] +=1\n","         if labels_features[i] == 6:\n","           n_count[-j,6] +=1\n","         if labels_features[i] == 7:\n","           n_count[-j,7] +=1\n","         if labels_features[i] == 8:\n","           n_count[-j,8] +=1\n","         if labels_features[i] == 9:\n","           n_count[-j,9] +=1\n","  \n","  pseudo_label = np.zeros(batch_size)\n","  err = 0      \n","\n","  # We extract the label for the new element\n","  for i in range(0,batch_size):\n","    pseudo_label[i]  =   np.argmax(n_count[i,:])  \n","    if pseudo_label[i]!= labels_features[i+int(n_cluster*len(labels_features_init)/10)]:\n","      err += 1\n","\n","  # Delete the last feature and the last label\n","  features = np.delete(features, len(features)-1, 0)\n","\n","  #elimino label che abbiamo concatenato\n","  labels_features = np.delete(labels_features, len(labels_features)-1, 0)\n","\n","  return pseudo_label, err"]},{"cell_type":"markdown","metadata":{"id":"zNrxFnGeWeYY"},"source":["# FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0DELLNfTtgF"},"outputs":[],"source":["def softmax(array):\n","    \n","    if(len(array.shape)==2):\n","        array = array[0]\n","        \n","    size    = len(array)\n","    ret_ary = np.zeros([len(array)])\n","    m       = array[0]\n","    sum_val = 0\n","\n","    for i in range(0, size):\n","        if(m<array[i]):\n","            m = array[i]\n","\n","    for i in range(0, size):\n","        sum_val += np.exp(array[i] - m)\n","\n","    constant = m + np.log(sum_val)\n","    for i in range(0, size):\n","        ret_ary[i] = np.exp(array[i] - constant)\n","        \n","    return ret_ary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCEnUmGuWeYY"},"outputs":[],"source":["class Custom_Layer(object):\n","    def __init__(self, model):\n","\n","        # Related to the layer\n","        self.ML_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n","        self.ML_frozen.compile()\n","        \n","        self.W = np.array(model.layers[-1].get_weights()[0])    # extract the weights from the last layer\n","        self.b = np.array(model.layers[-1].get_weights()[1])    # extract the biases from the last layer\n","               \n","        self.W_2 = np.zeros(self.W.shape)\n","        self.b_2 = np.zeros(self.b.shape)\n","        \n","        self.label     = [0,1,2,3,4,5]              \n","        self.std_label = [0,1,2,3,4,5,6,7,8,9]\n","        \n","        self.l_rate = 0                                         # learning rate that changes depending on the algorithm        \n","\n","        self.batch_size = 0\n","        \n","        # Related to the results fo the model\n","        self.conf_matr = np.zeros((10,10))    # container for the confusion matrix       \n","        self.macro_avrg_precision = 0       \n","        self.macro_avrg_recall = 0\n","        self.macro_avrg_F1score = 0\n","        \n","        self.title = ''       # title that will be displayed on plots\n","        self.filename = ''    # name of the files to be saved (plots, charts, conf matrix)\n","        \n","        \n","    # Function that is used for the prediction of the model saved in this class\n","    def predict(self, x):\n","        mat_prod = np.array(np.matmul(x, self.W) + self.b)\n","        return softmax(mat_prod) # othwerwise do it with keras|also remove np.array()| tf.nn.softmax(mat_prod) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6p72fbaTWeYZ"},"outputs":[],"source":["def digitToSoftmax(current_label, known_labels):\n","    ret_ary = np.zeros(len(known_labels))\n","\n","    known_labels_2 = [0,1,2,3,4,5]\n","                       \n","    for i in range(0, len(known_labels)):\n","        if(current_label == known_labels_2[i]):\n","            ret_ary[i] = 1\n","\n","    return ret_ary  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1V6LeFTVmv9"},"outputs":[],"source":["def NumberToSoftmax(current_label, known_labels):\n","    ret_ary = np.zeros(len(known_labels))\n","                       \n","    for i in range(0, len(known_labels)):\n","        if(current_label == known_labels[i]):\n","            ret_ary[i] = 1\n","\n","    return ret_ary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLtQr29AVn_Y"},"outputs":[],"source":["def CheckLabelKnown(model, current_label):\n","    \n","    found = 0\n","    \n","    for i in range(0, len(model.label)):\n","        if(current_label == model.label[i]):\n","            found = 1\n","        \n","        \n","    # If the label is not known\n","    if(found==0):\n","        print(f'\\n\\n    New digit detected -> digit \\033[1m{current_label}\\033[0m \\n')\n","\n","        model.label.append(current_label)   # Add new digit to label\n","                \n","        # Increase weights and biases dimensions\n","        model.W = np.hstack((model.W, np.zeros([model.W.shape[0],1])))\n","        model.b = np.hstack((model.b, np.zeros([1])))\n","        \n","        model.W_2 = np.hstack((model.W_2, np.zeros([model.W.shape[0],1])))\n","        model.b_2 = np.hstack((model.b_2, np.zeros([1])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXndowOwWeYZ"},"outputs":[],"source":["def trainOneEpoch_OL(model, x_test, y_test, features, labels_features, batch_size):\n","       \n","    cntr = 1\n","    learn_rate = model.l_rate\n","    \n","    test_samples = x_test.shape[0]\n","    \n","    n_cluster = 10\n","    err_tot = 0\n","    cntr_clus = 0\n","    reminder = test_samples%batch_size\n","    max_iter = int(test_samples//batch_size)\n","\n","    features_images = np.zeros(test_samples)\n","        \n","    # FEATURE EXTRACTION\n","\n","    print('**********************************\\n Performing features extraction \\n')\n","\n","    for i in range(0, test_samples):\n","        y_ML   = model.ML_frozen.predict(x_test[i].reshape((1,28,28,1)))\n","\n","        if i == 0:\n","          features_images = np.copy(y_ML)       \n","        else:\n","          features_images = np.concatenate((features_images, y_ML))\n","\n","    # CLUSTERING\n","\n","    print('**********************************\\n Performing clustering\\n')\n","\n","    # Pseudo-labels\n","    for i in range(0, max_iter):\n","      \n","        pseudo_label, err = k_mean_clustering(features_images[i*batch_size:i*batch_size+batch_size], features, y_test[i*batch_size:i*batch_size+batch_size], labels_features, n_cluster, batch_size)\n","        err_tot += err\n","        pseudo_label = pseudo_label.astype(int)\n","\n","        if i == 0:\n","          pseudo_labels = np.copy(pseudo_label)      \n","        else:\n","          pseudo_labels = np.append(pseudo_labels, pseudo_label)\n","\n","        print(f\"\\r    Currently at {np.round(np.round(cntr_clus/test_samples*batch_size,4)*100,2)}% of dataset\", end=\"\")\n","        print(\"\\n\")\n","        cntr_clus +=1\n","    \n","    # Pseudo-labels for last batch\n","    \n","    if reminder != 0: \n","        pseudo_label, err = k_mean_clustering(features_images[max_iter*batch_size:test_samples], features, y_test[max_iter*batch_size:test_samples], labels_features, n_cluster, reminder)\n","        err_tot = err_tot + err\n","        pseudo_labels = np.append(pseudo_labels, pseudo_label)\n","\n","    # Check errors in clustering\n","\n","    #print(\"The error in clustering is: \")\n","    #print(int(err_tot/test_samples*100))\n","    #print(\"%\")\n","    #print(\"\\n\")\n","\n","    # ONLINE-LEARNING\n","\n","    print('**********************************\\n Performing training with OL\\n')\n","\n","    for i in range(0, test_samples):\n","\n","        CheckLabelKnown(model, pseudo_labels[i])\n","    \n","        y_true_soft = NumberToSoftmax(pseudo_labels[i], model.label)\n","               \n","        # Prediction\n","        \n","        y_pred = model.predict(features_images[i,:])\n","        \n","        # Backpropagation\n","        cost = y_pred-y_true_soft\n","        \n","        for j in range(0,model.W.shape[0]):\n","\n","            # Update weights\n","            dW = np.multiply(cost, features_images[i,j]*learn_rate)\n","            model.W[j,:] = model.W[j,:]-dW\n","\n","        # Update biases\n","        db      = np.multiply(cost, learn_rate)\n","        model.b = model.b-db\n","        \n","    return y_pred"]},{"cell_type":"markdown","metadata":{"id":"rjrOJ9rSWeYc"},"source":["# TRAIN THE MODEL ON THE DIGITS 6-9 (OL METHOD)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iu-Jd60vWeYd","scrolled":false},"outputs":[],"source":["Model_OL = Custom_Layer(model)\n","Model_OL.title      = 'OL'\n","Model_OL.filename   = 'OL'\n","Model_OL.l_rate     = 0.01\n","Model_OL.batch_size = 8\n","\n","batch_size = 1\n","\n","prediction = trainOneEpoch_OL(Model_OL, '''qui le feautere da analizzare''', '''qui le labels delle feautere da analizzare''', features, labels_features, batch_size)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JNC3pztTWeYb"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"}}},"nbformat":4,"nbformat_minor":0}
