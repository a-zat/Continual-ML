{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Absolute path is needed to load libraries \n",
    "ROOT_PATH = os.path.abspath('')\n",
    "sys.path.append(ROOT_PATH + '/lib')\n",
    "\n",
    "from lib.CustomLayer_lib import Custom_Layer\n",
    "from lib.utils import *\n",
    "from lib.Kmeans_lib import cluster_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione V1: Creo Kmeans con labeled + unlabeled data - (implementazione attuale nel progetto)\n",
    "Per ogni batch unisco labeled e unlabeled data. Quindi creo ogni volta un clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "n_samples = 100\n",
    "\n",
    "# Load model and features\n",
    "n_feat = 10 # Select number of features\n",
    "MODEL_PATH = 'Models/{}/'.format(n_feat)\n",
    "features_saved = np.loadtxt(MODEL_PATH + 'll_features.txt')\n",
    "labels_saved = np.loadtxt(MODEL_PATH + 'll_labels_features.txt').astype(int)\n",
    "keras_model = keras.models.load_model(MODEL_PATH + 'original_mnist_cnn.h5') # Original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and extract the features for the labels we need\n",
    "\n",
    "# Define initial set of features\n",
    "# labels_init_list = list([1, 9, 5, 0])\n",
    "# labels_init_list = list(range(0,9))\n",
    "labels_init_list = model.std_label\n",
    "n_cluster = len(labels_init_list)\n",
    "\n",
    "# Extract from the saved features the labels that we need\n",
    "features_saved_init = []\n",
    "labels_saved_init = []\n",
    "# Extract features of digits considered in labels_init_list\n",
    "for i in range(0, len(features_saved)):\n",
    "    if labels_saved[i] in labels_init_list:\n",
    "      features_saved_init.append(features_saved[i,:])\n",
    "      labels_saved_init.append(labels_saved[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset, and add the concatenate the saved and new features together\n",
    "digits_run, labels_run, _, _ = create_dataset(n_samples, 0)\n",
    "model = Custom_Layer(keras_model)\n",
    "features_run = model.ML_frozen.predict(digits_run.reshape((n_samples,28,28,1)), verbose = False)\n",
    "\n",
    "# Convert list to nparray\n",
    "features = np.array(features_saved_init)\n",
    "features = features.astype('float32')\n",
    "labels_features = np.array(labels_saved_init)  \n",
    "\n",
    "# Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n",
    "features = np.concatenate((features, features_run))\n",
    "labels_features = np.append(labels_features, labels_run).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMean Clustering\n",
    "k = KMeans(n_cluster, n_init=100)\n",
    "k.fit(features)\n",
    "\n",
    "# Find pseudolabels for each new image\n",
    "# Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n",
    "clusters_features_saved = list(k.labels_[0:len(labels_saved_init)])\n",
    "cluster_list = list(range(0,n_cluster))\n",
    "map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_features_saved, list(labels_saved_init), cluster_list, labels_init_list, verbose = (model.settings.verbosity == 'DEBUG'))\n",
    "\n",
    "clusters_features = k.labels_\n",
    "\n",
    "# Compute pseudolabels\n",
    "pseudolabels = []\n",
    "for i in range(0, len(clusters_features)):\n",
    "  pseudolabel = map_clu2lbl[clusters_features[i]]\n",
    "  pseudolabels.append(pseudolabel)\n",
    "\n",
    "pseudolabels_run = pseudolabels[len(clusters_features) - len(labels_run): len(clusters_features)]\n",
    "\n",
    "err = 0 # Initialize error counter\n",
    "for i in range(len(labels_run)):\n",
    "  if pseudolabels_run[i] != labels_run[i]:\n",
    "    err += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione V2: Comparo due Kmeans - (funzionante)\n",
    "Runno un Kmeans sulle saved features e uno sulle nuove. Comparo le distanze tra i centroidi per determinare la corrispondenza tra i cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "n_feat = 50 # Select number of features\n",
    "n_samples = 100\n",
    "\n",
    "# Load model and features\n",
    "MODEL_PATH = 'Models/{}/'.format(n_feat)\n",
    "features_saved = np.loadtxt(MODEL_PATH + 'll_features.txt')\n",
    "labels_saved = np.loadtxt(MODEL_PATH + 'll_labels_features.txt').astype(int)\n",
    "keras_model = keras.models.load_model(MODEL_PATH + 'original_mnist_cnn.h5') # Original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create Kmeans with the saved features\n",
    "n_cluster = 10\n",
    "k1 = KMeans(n_cluster, n_init=100)\n",
    "k1.fit(features_saved)\n",
    "\n",
    "k1.cluster_centers_.shape\n",
    "\n",
    "# Map cluster to labels\n",
    "map_clu2lbl, map_lbl2clu = cluster_to_label(k1.labels_, labels_saved, list(range(0,n_cluster)), model.std_label)\n",
    "\n",
    "map_clu2lbl\n",
    "\n",
    "# Per migliorare questa parte sarebbe da riuscire a creare i cluster usando le labels dato che sono note. Così non abbiamo errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kmeans with the new features\n",
    "# Create dataset\n",
    "digits_run, labels_run, _, _ = create_dataset(n_samples, 0)\n",
    "model = Custom_Layer(keras_model)\n",
    "features_run = model.ML_frozen.predict(digits_run.reshape((n_samples,28,28,1)), verbose = False)\n",
    "\n",
    "n_clusters = 10\n",
    "k2 = KMeans(n_clusters, n_init=100)\n",
    "k2.fit(features_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros([10,10])\n",
    "\n",
    "for i in range(0, n_cluster):\n",
    "    matrix[i,:] = k2.transform(k1.cluster_centers_[i,:].reshape(1, -1))\n",
    "\n",
    "# Rows = distances of center k1_i to the centers of k_2\n",
    "\n",
    "np.around(matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map cluster to cluster (argmin)\n",
    "\n",
    "# Find max in each row -> cluster corresponding to each label\n",
    "argmin_axis0 = np.argmin(matrix, axis = 0) # Min of each col \n",
    "argmin_axis1 = np.argmin(matrix, axis = 1) # Min of each row\n",
    "\n",
    "print(argmin_axis0, argmin_axis1)\n",
    "print(set(argmin_axis0), set(argmin_axis1))\n",
    "\n",
    "# Using argmin axis = 1 seems better\n",
    "map_idx = argmin_axis0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill dictionary with map\n",
    "map_k1_2_k2 = {}\n",
    "map_k2_2_k1 = {}\n",
    "\n",
    "for i in range(0, len(map_idx)):\n",
    "  map_k1_2_k2[map_idx[i]] = model.std_label[i]\n",
    "  map_k2_2_k1[model.std_label[i]] = map_idx[i]\n",
    "\n",
    "print(np.around(matrix, 1))\n",
    "print(\"Argmax:\", map_idx)\n",
    "print(\"Cluster K1 (saved) to K2 (new) map: \", map_k1_2_k2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pseudolabels\n",
    "pseudolabels = np.zeros(n_samples)\n",
    "errs = 0\n",
    "for i in range(0, n_samples):\n",
    "    pseudolabels[i] = map_clu2lbl[map_k2_2_k1[k2.labels_[i]]]\n",
    "    if pseudolabels[i] != labels_run[i]:\n",
    "        errs += 1 \n",
    "        print(\"True label:\", labels_run[i],\"Pseudolabel:\", pseudolabels[i],\"K2 cluster:\", k2.labels_[i], \"K1-mapped cluster:\", map_k2_2_k1[k2.labels_[i]])\n",
    "\n",
    "# print(pseudolabels)\n",
    "print(\"error:\", errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione V3: Modifica di V1 che però aggiorna i cluster anzichè ricrearli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "n_samples = 10000\n",
    "\n",
    "# Load model and features\n",
    "n_feat = 10 # Select number of features\n",
    "MODEL_PATH = 'Models/{}/'.format(n_feat)\n",
    "features_saved = np.loadtxt(MODEL_PATH + 'll_features.txt')\n",
    "labels_saved = np.loadtxt(MODEL_PATH + 'll_labels_features.txt').astype(int)\n",
    "keras_model = keras.models.load_model(MODEL_PATH + 'original_mnist_cnn.h5') # Original model \n",
    "\n",
    "# Create dataset, and add the concatenate the saved and new features together\n",
    "digits_run, labels_run, _, _ = create_dataset(n_samples, 0)\n",
    "model = Custom_Layer(keras_model)\n",
    "features_run = model.ML_frozen.predict(digits_run.reshape((n_samples,28,28,1)), verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco labels set\n",
    "labels_init_list = model.std_label\n",
    "n_cluster = len(labels_init_list)\n",
    "\n",
    "# Extract from the saved features the labels that we need\n",
    "features_saved_init = []\n",
    "labels_saved_init = []\n",
    "# Extract features of digits considered in labels_init_list\n",
    "for i in range(0, len(features_saved)):\n",
    "    if labels_saved[i] in labels_init_list:\n",
    "        features_saved_init.append(features_saved[i,:])\n",
    "        labels_saved_init.append(labels_saved[i])\n",
    "\n",
    "# Convert list to nparray\n",
    "features = np.array(features_saved_init)\n",
    "labels_features = np.array(labels_saved_init)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un dizionario per linkare le features (salvate) al cluster di appartenenza\n",
    "# creates dictionary using dictionary comprehension -> list [] is mutable object\n",
    "features_saved_dict = { key : [] for key in labels_init_list}\n",
    "\n",
    "for i in range(0, len(features_saved_init)):\n",
    "    lbl = labels_saved_init[i]\n",
    "    features_saved_dict[lbl].append(features_saved_init[i])\n",
    "\n",
    "# print(features_saved_dict[2][1][230])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco i centroidi iniziali facendo una media ndei samples nel cluster.\n",
    "# cluster_mean_dict = { key : [] for key in labels_init_list}\n",
    "cluster_mean = []\n",
    "# Converto list-of-arrays in 2D array\n",
    "for key in labels_init_list:\n",
    "  features_saved_dict[key] = np.array(features_saved_dict[key])\n",
    "  cluster_mean.append(np.mean(features_saved_dict[key], axis=0))\n",
    "\n",
    "cluster_mean = np.array(cluster_mean)\n",
    "# print(cluster_mean.shape)\n",
    "\n",
    "# Create KMeans\n",
    "kmeans = KMeans(n_cluster)\n",
    "kmeans.fit(cluster_mean)\n",
    "map_clu2lbl, map_lbl2clu = cluster_to_label(kmeans.labels_, labels_init_list, list(range(0,n_cluster)), model.std_label)\n",
    "\n",
    "# print(kmeans.predict(cluster_mean))\n",
    "print(\"Map cluster to label:\", map_clu2lbl)\n",
    "\n",
    "# Passo una nuova immagine al Kmeans. Ne determino il cluster e ne calcolo la pseudolabel\n",
    "errs = 0\n",
    "cluster_label = np.zeros(n_samples, dtype=int)\n",
    "for i in range(0, n_samples):\n",
    "    labels_new = labels_run[i]\n",
    "    features_new = np.array(features_run[i,:], dtype = type(features_saved[0,0]))\n",
    "\n",
    "    # Find the cluster for the new features\n",
    "    cluster_label[i] = kmeans.predict(features_new.reshape(1, -1))\n",
    "    pseudolabel = map_clu2lbl[cluster_label[i]]\n",
    "\n",
    "    if labels_new != pseudolabel:\n",
    "        # print(\"True label:\", labels_new,\"Pseudolabel:\", pseudolabel, \"Index:\", i)\n",
    "        errs += 1\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"CORRECT!!!\", \"True label:\", labels_new,\"Pseudolabel:\", pseudolabel, \"Index:\")\n",
    "\n",
    "    # Update the cluster center\n",
    "    l_rate = 0.02\n",
    "    kmeans.cluster_centers_[cluster_label[i],:] = (kmeans.cluster_centers_[cluster_label[i],:] + features_new * l_rate)/(1 + l_rate)\n",
    "    # print(cluster_label[i])\n",
    "\n",
    "print(\"Errors:\", errs, \"Accuracy: {:.1%}\".format(1- errs/n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora sarebbe da capire quando bisogna aggiungere un nuovo centroide, osservando le metriche interne\n",
    "\n",
    "#   kmeans.score()\n",
    "#   kmeans.transform() \n",
    "#   sklearn metrics -> es. silhouette\n",
    "# \n",
    "# Idea: faccio il clustering variando il numero di cluster e vedo di minimizzare la distanza.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
