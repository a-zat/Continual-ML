{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import random \n",
    "from random import seed\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib import Kmeans_lib\n",
    "from lib.Kmeans_lib import *\n",
    "from lib.EvalMetrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load features and labels\n",
    "features_saved = np.loadtxt('Models/Original_model/ll_features_20.txt')\n",
    "labels_features_saved = np.loadtxt('Models/Original_model/ll_labels_features_20.txt').astype(int)\n",
    "\n",
    "\n",
    "# Load model - CFR\n",
    "model = keras.models.load_model('Models/Original_model/mnist_cnn.h5') # Frozen model \n",
    "\n",
    "# model_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n",
    "# model_frozen.compile()\n",
    "\n",
    "model_frozen = keras.models.load_model('Models/Frozen_model/omv_mnist_cnn.h5')\n",
    "\n",
    "# Print\n",
    "# model.summary()\n",
    "# model_frozen.summary()\n",
    "\n",
    "# Absolute path is needed to load libraries \n",
    "ROOT_PATH = os.path.abspath('')\n",
    "sys.path.append(ROOT_PATH + '/lib')\n",
    "\n",
    "\n",
    "\n",
    "# Test import\n",
    "# from lib import simulation_lib\n",
    "# from lib.simulation_lib import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import serial.tools.list_ports\n",
    "import serial, struct\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "import glob\n",
    "from keras import applications\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "from random import seed\n",
    "\n",
    "from numpy.ma.core import size\n",
    "\n",
    "import sys, os\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(0, ROOT_PATH + '/lib')\n",
    "\n",
    "from importMnist import createDataset\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#    _______  ______  _        _    _   _    _  _____ ___ ___  _   _\n",
    "#   | ____\\ \\/ /  _ \\| |      / \\  | \\ | |  / \\|_   _|_ _/ _ \\| \\ | |\n",
    "#   |  _|  \\  /| |_) | |     / _ \\ |  \\| | / _ \\ | |  | | | | |  \\| |\n",
    "#   | |___ /  \\|  __/| |___ / ___ \\| |\\  |/ ___ \\| |  | | |_| | |\\  |\n",
    "#   |_____/_/\\_\\_|   |_____/_/   \\_\\_| \\_/_/   \\_\\_| |___\\___/|_| \\_|\n",
    "\n",
    "\"\"\"\n",
    "This python script is used for sincronizing the OpenMV camera and the laptop during training. The idea is to disaply digits images on the \n",
    "laptop screen and at the same time send throught the UART (usb cable) to the OpenMV camera the correct label of the image displayed.\n",
    "This should allow the camera to have the true label and correctly compute the error and later perform the backpropagation on biases and weights \n",
    "in order to perform the OL training.\n",
    "\n",
    "Note that the UART on the USB cable is usually occupied by the OpenMV IDE, which will use this cable for receiving all the debugging informations from\n",
    "the OpenMV camera (such as the video stream). In order to be able to communicate the informations from the Laptop to the OpenMV camera it is necessary to\n",
    "flash the MicroPython code on the camera as the main.py script (in the IDE go to Tools->Save opened scipt as main.py). In this way, any time the camera\n",
    "is powered on and NOT connected in debugging mode to the IDE (in the IDE in the bottom left corner you should see the disconnected image), the main.py script\n",
    "is ran automatically and is possible to use the UART connection for sending and receiveing data (also the photos taken from the camera can be sent to the \n",
    "PC but for sure the code will be slower and it can easily get out of sync).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "############################################################\n",
    "#    _____ _   _ _   _  ____ _____ ___ ___  _   _ ____\n",
    "#   |  ___| | | | \\ | |/ ___|_   _|_ _/ _ \\| \\ | / ___|\n",
    "#   | |_  | | | |  \\| | |     | |  | | | | |  \\| \\___ \\\n",
    "#   |  _| | |_| | |\\  | |___  | |  | | |_| | |\\  |___) |\n",
    "#   |_|    \\___/|_| \\_|\\____| |_| |___\\___/|_| \\_|____/\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# DEFINITION OF THE FUNCTIONS USED FOR THE OPENMV\n",
    "##################################################################\n",
    "\n",
    "def on_change(val):\n",
    "    # Function called when the sliding bar changes value.                                            #\n",
    "    # The function changes the value TRAINING FLAG that is used as a indicator of the state machine. #\n",
    "    # Parameters                                                                                     #\n",
    "    # ..........                                                                                     #\n",
    "    # val : integer                                                                                  #\n",
    "    # Is the value set by the sliding bar                                                            #\n",
    "\n",
    "    myClass.TRAINING_FLAG = val\n",
    "    if (val == 0):\n",
    "        print('Script is in IDLE MODE')\n",
    "    elif (val == 1):\n",
    "        print('Script is in STREAMING MODE')\n",
    "    elif (val == 2):\n",
    "        print('Script is in STREAMING ELABORATION MODE')\n",
    "    elif (val == 3):\n",
    "        print('Script is in TRAINING MODE')\n",
    "\n",
    "\n",
    "\n",
    "class uselessContainer():\n",
    "    # Container that I use because I need to change the parameter TRAINING_FLAG            #\n",
    "    # if I don't use a class the value is not changed by ID and the script never           #\n",
    "    # updates the real value but creates a new value with the same name but different ID   #\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.TRAINING_FLAG = 3\n",
    "        self.cont = 0 \n",
    "\n",
    "##################################################################\n",
    "\n",
    "def k_mean_clustering(features_x_test, features_init, y_test, labels_features_init, n_cluster, batch_size):\n",
    "\n",
    "    cntr = 0\n",
    "    err = 0\n",
    "\n",
    "    n_count = np.zeros((batch_size, n_cluster)) #inizializzo un vettore che ci permette di contare per ogni cluster quali numeri sono stati inseriti\n",
    "    pseudo_label = np.zeros(batch_size)\n",
    " \n",
    "    # creo due vettori vuoti uno per le label lungo (n_features) l'altro per le features lungo (n_features,512)\n",
    "    features = np.zeros((int(n_cluster*len(labels_features_init)/10),512))\n",
    "    labels_features = np.zeros(int(n_cluster*len(labels_features_init)/10))\n",
    "\n",
    "    #passo il vettore salvato nel file e mi tiro fuori solo i numeri che mi interessano. Se ho 6 cluster, mi interessano solo gli 0, 1, 2, 3, 4 e 5.\n",
    "    for i in range(0,len(features_init)-1):\n",
    "        if labels_features_init[i] < n_cluster:\n",
    "            features[cntr,:] = features_init[i,:]\n",
    "            labels_features[cntr] = labels_features_init[i]\n",
    "            cntr += 1\n",
    "\n",
    "    # aggiungo la feature che di cui ci interessa trovare la pseudo label\n",
    "    features = np.concatenate((features, features_x_test))\n",
    "    labels_features = np.append(labels_features, y_test)\n",
    "\n",
    "    # viene fatto il clustering\n",
    "    k_mean = create_k_mean(features ,n_cluster)\n",
    "\n",
    "    # qui viene riempito il vettore conteggio. Se l'immagine che ci interessa classificare è stata inserita nel cluster 0, il ciclo guarderà \n",
    "    # tutte le labels delle immagini che sono state messe in quel cluster e la label che compare più volte sarà la pseudo_label della nostra immagine\n",
    "    for j in range(1, batch_size+1):\n",
    "        for i in range(0,len(k_mean.labels_) - batch_size):\n",
    "            if k_mean.labels_[i] == k_mean.labels_[-j]:\n",
    "                if labels_features[i] == 0:\n",
    "                    n_count[-j,0] +=1\n",
    "                if labels_features[i] == 1:\n",
    "                    n_count[-j,1] +=1\n",
    "                if labels_features[i] == 2:\n",
    "                    n_count[-j,2] +=1\n",
    "                if labels_features[i] == 3:\n",
    "                    n_count[-j,3] +=1\n",
    "                if labels_features[i] == 4:\n",
    "                    n_count[-j,4] +=1\n",
    "                if labels_features[i] == 5:\n",
    "                    n_count[-j,5] +=1\n",
    "                if labels_features[i] == 6:\n",
    "                    n_count[-j,6] +=1\n",
    "                if labels_features[i] == 7:\n",
    "                    n_count[-j,7] +=1\n",
    "                if labels_features[i] == 8:\n",
    "                    n_count[-j,8] +=1\n",
    "                if labels_features[i] == 9:\n",
    "                    n_count[-j,9] +=1\n",
    "\n",
    "\n",
    "    # We extract the label for the new element\n",
    "    for i in range(0,batch_size):\n",
    "        pseudo_label[i]  =   np.argmax(n_count[i,:])\n",
    "        pseudo_label[i] = pseudo_label[i].astype(int)  \n",
    "        if pseudo_label[i]!= labels_features[i+int(n_cluster*len(labels_features_init)/10)]:\n",
    "            err += 1\n",
    "\n",
    "    return pseudo_label, err\n",
    "\n",
    "\n",
    "def trainOneEpoch_CWR(model, x_test, y_test, features, labels_features, clust_batch_size, found_digit):\n",
    "    \n",
    "    learn_rate = model.l_rate\n",
    "    model_batch_size = model.batch_size\n",
    "\n",
    "    test_samples = x_test.shape[0]\n",
    "    \n",
    "    n_cluster = 10\n",
    "    \n",
    "    err_clu = 0\n",
    "    err_mod = 0\n",
    "    \n",
    "    prediction_vec = np.zeros(test_samples)\n",
    "\n",
    "    # CLUSTERING\n",
    "    print('**********************************\\n Performing clustering\\n')\n",
    "    \n",
    "    # Pseudo-labels\n",
    "\n",
    "    pseudo_labels, err_clu = k_mean_clustering(x_test, features, y_test, labels_features, n_cluster, clust_batch_size)\n",
    "\n",
    "    # ONLINE-LEARNING\n",
    "    print('**********************************\\nPerforming training CWR \\n ')  \n",
    "                   \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, test_samples):\n",
    "            \n",
    "        CheckLabelKnown(model, pseudo_labels[i])\n",
    "        y_true_soft = NumberToSoftmax(pseudo_labels[i], model.label)\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1] \n",
    "\n",
    "        found_digit[np.argmax(y_true_soft)] += 1  # update the digit counter\n",
    "            \n",
    "        # PREDICTION\n",
    "\n",
    "        y_pred_c = softmax(np.array(np.matmul(x_test[i,:], model.W) + model.b))      \n",
    "        y_pred_t = softmax(np.array(np.matmul(x_test[i,:], model.W_2) + model.b_2)) \n",
    "        \n",
    "        prediction_vec[i] = np.argmax(y_pred_c)\n",
    "\n",
    "        # Error\n",
    "        if(prediction_vec[i] !=  y_test[i]):\n",
    "        #if(np.argmax(y_pred) !=  y_test):  \n",
    "            err_mod += 1\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred_t-y_true_soft\n",
    "\n",
    "        # Update weights\n",
    "        for j in range(0,h):\n",
    "            deltaW = np.multiply(cost, x_test[i,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W_2[j,:] = model.W_2[j,:] - dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b_2 = model.b_2-db\n",
    "        \n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%model_batch_size==0 and i!=0): \n",
    "            for k in range(0, w):\n",
    "                if(found_digit[k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_digit[k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_digit[k])\n",
    "                    model.W[:,k] = np.multiply(tempW+model.W_2[:,k], 1/(found_digit[k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+model.b_2[k],   1/(found_digit[k]+1))\n",
    "                    \n",
    "            model.W_2  =  np.copy(model.W) # np.zeros((model.W.shape)) \n",
    "            model.b_2  =  np.copy(model.b) # np.zeros((model.b.shape))       \n",
    "            found_digit = np.zeros(10)  # reset\n",
    "    \n",
    "    return pseudo_labels, prediction_vec, err_clu, err_mod\n",
    "\n",
    "\n",
    "def trainOneEpoch_LWF(model, x_test, y_test, features, labels_features, clust_batch_size):\n",
    "    \n",
    "    learn_rate = model.l_rate\n",
    "    model_batch_size = model.batch_size\n",
    "\n",
    "    test_samples = x_test.shape[0]\n",
    "    \n",
    "    n_cluster = 10\n",
    "    \n",
    "    err_clu = 0\n",
    "    err_mod = 0\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    \n",
    "    found_digit = np.zeros(10)\n",
    "    \n",
    "    prediction_vec = np.zeros(test_samples)\n",
    "      \n",
    "    \n",
    "    # CLUSTERING\n",
    "    print('**********************************\\n Performing clustering\\n')\n",
    "    \n",
    "    # Pseudo-labels\n",
    "\n",
    "    pseudo_labels, err_clu = k_mean_clustering(x_test, features, y_test, labels_features, n_cluster, clust_batch_size)\n",
    "\n",
    "    # ONLINE-LEARNING\n",
    "\n",
    "    print('**********************************\\nPerforming training with LWF - STOCHASTIC\\n ') \n",
    "    \n",
    "    y_LWF = np.zeros([1, 8])    # Define container for LWF\n",
    "\n",
    "    # DEFINE ORIGINAL WEIGHTS AND BIASES\n",
    "    model.W_2 = np.copy(model.W)\n",
    "    model.b_2 = np.copy(model.b)\n",
    "         \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, test_samples):\n",
    "                \n",
    "        CheckLabelKnown(model, pseudo_labels[i] )\n",
    "        y_true_soft = NumberToSoftmax(pseudo_labels[i], model.label)\n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "\n",
    "        # PREDICTION WITH CURRENT WEIGHTS\n",
    "     \n",
    "        y_pred = model.predict(x_test[i,:])\n",
    "\n",
    "        # PREDICTION WITH LWF WEIGHTS\n",
    "        y_LWF = myFunc_softmax(np.array(np.matmul(y_ML, model.W_2) + model.b_2))\n",
    "        \n",
    "        lam = 100/(100+cntr)   \n",
    "        \n",
    "        # BACKPROPAGATION        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF\n",
    "\n",
    "\n",
    "        for j in range(0,h):\n",
    "            # Update weights\n",
    "            deltaW_norm  = np.multiply(cost_norm,1-lam)\n",
    "            deltaW_LWF   = np.multiply(cost_LWF, lam)\n",
    "            dW           = np.multiply(deltaW_norm+deltaW_LWF, y_ML[0,j]*learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db_norm = np.multiply(cost_norm, 1-lam)\n",
    "        db_LWF  = np.multiply(cost_LWF, lam)\n",
    "        db      = np.multiply(db_norm+db_LWF, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1\n",
    "    \n",
    "    \n",
    "    return pseudo_labels, prediction_vec, err_clu, err_mod\n",
    "\n",
    "def trainOneEpoch_OL(model, x_test, y_test, features, labels_features, batch_size):\n",
    "    \n",
    "    learn_rate = model.l_rate\n",
    "\n",
    "    test_samples = x_test.shape[0]\n",
    "    \n",
    "    n_cluster = 10\n",
    "    \n",
    "    err_clu = 0\n",
    "    err_mod = 0\n",
    "    \n",
    "    max_iter = int(test_samples//batch_size)\n",
    "\n",
    "    prediction_vec = np.zeros(test_samples)\n",
    "      \n",
    "    \n",
    "    # CLUSTERING\n",
    "    print('**********************************\\n Performing clustering\\n')\n",
    "    \n",
    "    # Pseudo-labels\n",
    "\n",
    "    pseudo_labels, err_clu = k_mean_clustering(x_test, features, y_test, labels_features, n_cluster, batch_size)\n",
    "\n",
    "    # ONLINE-LEARNING\n",
    "    print('**********************************\\n Performing training with OL\\n')\n",
    "\n",
    "    for i in range(0, test_samples):\n",
    "\n",
    "        CheckLabelKnown(model, pseudo_labels[i])\n",
    "    \n",
    "        y_true_soft = NumberToSoftmax(pseudo_labels[i], model.label)\n",
    "               \n",
    "        # Prediction\n",
    "        y_pred = model.predict(x_test[i,:])\n",
    "        prediction_vec[i] = np.argmax(y_pred)\n",
    "        \n",
    "        # Error\n",
    "        if(prediction_vec[i] !=  y_test[i]):\n",
    "        #if(np.argmax(y_pred) !=  y_test):\n",
    "            err_mod += 1\n",
    "        \n",
    "        # Backpropagation\n",
    "        cost = y_pred-y_true_soft\n",
    "        \n",
    "        for j in range(0,model.W.shape[0]):\n",
    "\n",
    "            # Update weights\n",
    "            dW = np.multiply(cost, x_test[i,j]*learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db      = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "\n",
    "    \n",
    "    #y_true_soft = NumberToSoftmax(y_test, model.label)\n",
    "                   \n",
    "    # Find the max iter for both true label and prediction\n",
    "    #if(np.amax(y_true_soft) != 0):\n",
    "    #    max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "    #if(np.amax(y_pred) != 0):\n",
    "    #    max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "    # Fill up the confusion matrix\n",
    "    #for k in range(0,len(model.label)):\n",
    "    #    if(model.label[max_i_pred] == model.std_label[k]):\n",
    "    #        p = np.copy(k)\n",
    "    #    if(model.label[max_i_true] == model.std_label[k]):\n",
    "    #        t = np.copy(k)\n",
    "\n",
    "    #model.conf_matr[t,p] += 1 \n",
    "    \n",
    "    return pseudo_labels, prediction_vec, err_clu, err_mod\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "#    __  __    _    ___ _   _\n",
    "#   |  \\/  |  / \\  |_ _| \\ | |\n",
    "#   | |\\/| | / _ \\  | ||  \\| |\n",
    "#   | |  | |/ ___ \\ | || |\\  |\n",
    "#   |_|  |_/_/   \\_\\___|_| \\_|\n",
    "\n",
    "# Loop for saving new features\n",
    "#for z in range(0,10):\n",
    "\n",
    "#################################################\n",
    "# PARAMETERS FOR IMPLEMENTATION OF ACTIVE MODEL\n",
    "#################################################\n",
    "\n",
    "# Path of the images to open\n",
    "ROOT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "myClass = uselessContainer()  # Init the class that stores the state of the camera\n",
    "\n",
    "# Open serial port\n",
    "# Next lines are taken from the example script in the OpenMV IDE - the example is in    File->Examples->OpenMV->Board Control->usb_vcp.py\n",
    "# NB: see the name of the com port used from the camera in                              Windows->Device manager->Ports(COM and LPT)\n",
    "port = '/dev/tty.usbmodem3067376B30301'\n",
    "sp = serial.Serial(port, baudrate=115200, bytesize=serial.EIGHTBITS, parity=serial.PARITY_NONE, xonxoff=False,\n",
    "                rtscts=False, stopbits=serial.STOPBITS_ONE, timeout=5000, dsrdtr=True)\n",
    "sp.setDTR(True)\n",
    "\n",
    "# Import the dataset that I am going to display\n",
    "samples_for_each_digit = 400\n",
    "digits_i_want = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#digits_i_want = [z]\n",
    "\n",
    "digits_data, digits_label = createDataset(samples_for_each_digit, digits_i_want)  # load dataset ; originally was createDataset(samples_for_each_digit + 1, digits_i_want)\n",
    "tot_samples = len(digits_label)                                                   # original is len(digits_label) - 8\n",
    "\n",
    "testing_number = 500\n",
    "training_number = tot_samples - testing_number\n",
    "\n",
    "#########################################################\n",
    "# EXTRA PARAMETERS FOR IMPLEMENTATION OF ACTIVE MODEL\n",
    "#########################################################\n",
    "\n",
    "ref_feat = np.loadtxt('ll_feat_50.txt')\n",
    "\n",
    "labels_features = np.loadtxt('ll_lab_feat_50.txt')\n",
    "\n",
    "model = keras.models.load_model('mnist_cnn.h5')\n",
    "\n",
    "out_collect = []\n",
    "\n",
    "batch_size = 50\n",
    "err_cluster = 0\n",
    "err_model = 0\n",
    "found_digit = np.zeros(10)\n",
    "cntr = 1\n",
    "cntr_batch = 1\n",
    "\n",
    "Model_OL = Custom_Layer(model)\n",
    "Model_OL.title      = 'OL'\n",
    "Model_OL.filename   = 'OL'\n",
    "Model_OL.W_2  = np.zeros((Model_OL.W.shape))\n",
    "Model_OL.b_2  = np.zeros((Model_OL.b.shape))\n",
    "#Model_OL.W_2  = np.copy((Model_OL.W))\n",
    "#Model_OL.b_2  = np.copy((Model_OL.b))\n",
    "Model_OL.l_rate     = 0.01\n",
    "Model_OL.batch_size = 10\n",
    "\n",
    "\n",
    "######################\n",
    "# START OF THE CODE\n",
    "######################\n",
    "\n",
    "print('\\n\\n ***** EVERYTHING IS LOADED - READY TO RUN ***** \\n\\n')\n",
    "\n",
    "while 1:\n",
    "    print('\\n now i am in the loop\\n')\n",
    "\n",
    "    # Show digit/idle message\n",
    "    if (myClass.TRAINING_FLAG != 0):\n",
    "        zoom_digit = cv2.resize(digits_data[cntr-1], (0, 0), fx=7, fy=7)\n",
    "        cv2.imshow('SYNC APP', zoom_digit)\n",
    "        cv2.waitKey(150)\n",
    "\n",
    "\n",
    "    # Send cmd + label to OpenMV\n",
    "    if (myClass.TRAINING_FLAG == 0 or myClass.TRAINING_FLAG == 1):\n",
    "        sp.write(b\"snap\")  # the camera will be in streaming mode\n",
    "        sp.flush()\n",
    "\n",
    "        # Receive image from OpenMV - careful it's easy to get out of sync\n",
    "        size = struct.unpack('<L', sp.read(4))[0]\n",
    "        img_raw = sp.read(size)\n",
    "        img_openmv = cv2.imdecode(np.frombuffer(img_raw, np.uint8), cv2.IMREAD_COLOR)\n",
    "        zoom_openmv = cv2.resize(img_openmv, (0, 0), fx=5, fy=5)\n",
    "        cv2.imshow('OpenMV view - Zoomed', zoom_openmv)\n",
    "        cv2.waitKey(150)\n",
    "\n",
    "\n",
    "    elif (myClass.TRAINING_FLAG == 2):\n",
    "        sp.write(b\"elab\")  # the camera will be in streaming mode\n",
    "        sp.flush()\n",
    "        \n",
    "        # Receive image from OpenMV - careful it's easy to get out of sync\n",
    "        size = struct.unpack('<L', sp.read(4))[0]\n",
    "        img_raw = sp.read(size)\n",
    "        img_openmv = cv2.imdecode(np.frombuffer(img_raw, np.uint8), 0)\n",
    "        zoom_openmv = cv2.resize(img_openmv, (400, 400))\n",
    "        cv2.imshow('OpenMV view - Zoomed', zoom_openmv)\n",
    "        cv2.waitKey(150)\n",
    "\n",
    "        \n",
    "    elif (myClass.TRAINING_FLAG == 3):\n",
    "\n",
    "        if cntr == 1:\n",
    "            cv2.waitKey(8000)\n",
    "        \n",
    "        sp.write(b\"trai\")  # the camera will train on the image taken\n",
    "        sp.flush()\n",
    "\n",
    "        print(f'counter: ',cntr,'/',tot_samples,'\\n')\n",
    "\n",
    "        feature = sp.read(3072).decode(\"utf-8\")\n",
    "        sp.flush()\n",
    "\n",
    "        # This section is a post processing in case of firmware.bin implementation on OpenMV.      #\n",
    "        # Sometimes the converted features are bigger than 512 and I have to remove elements.      #\n",
    "        # I choose void strings and strings containing only integer elements inside to be removed  #\n",
    "        \n",
    "        feature = feature[0:len(feature)-2].split(',')\n",
    "        feature_new = (np.asarray([float(i) for i in feature])).reshape(1,len(feature))\n",
    "\n",
    "        # This is section is for generating new features                                           #\n",
    "        # The vector of the current feature is saved in a vector that contains, at the end of the  #\n",
    "        # all the fetures of the same digit.                                                       #    \n",
    "        \n",
    "        #if cntr == 1:\n",
    "        #    feature_file_0 = np.copy(feature_new)\n",
    "        #    label_file_0 = np.copy(digits_label[cntr-1])\n",
    "        #if cntr > 1:\n",
    "        #    feature_file_0 = np.concatenate((feature_file_0,feature_new))\n",
    "        #    label_file_0 = np.append(label_file_0,digits_label[cntr-1])\n",
    "        \n",
    "        # Generation of the batch vector   \n",
    "                                                            \n",
    "        if cntr_batch == 1:\n",
    "            feature_batch = np.copy(feature_new)\n",
    "            label_batch= np.copy(digits_label[cntr-1])\n",
    "        if cntr_batch > 1:\n",
    "            feature_batch= np.concatenate((feature_batch,feature_new))\n",
    "            label_batch = np.append(label_batch,digits_label[cntr-1])\n",
    "\n",
    "        # Elaboration\n",
    "        if(cntr_batch == batch_size):\n",
    "            \n",
    "            # Make labels integers and rounds to 3 decimals the features\n",
    "            label_batch = label_batch.astype(int)\n",
    "            labels_features = labels_features.astype(int)\n",
    "            #feature_batch = np.round(feature_batch,3)\n",
    "            \n",
    "            #pseudo_labels, predictions, err_clu, err_mod = trainOneEpoch_OL(Model_OL, feature_batch, label_batch, ref_feat, labels_features, batch_size)\n",
    "            pseudo_labels, predictions, err_clu, err_mod = trainOneEpoch_CWR(Model_OL, feature_batch, label_batch, ref_feat, labels_features, batch_size, found_digit)\n",
    "\n",
    "            # Error calculation\n",
    "            if cntr > training_number:\n",
    "                err_cluster = err_cluster + err_clu\n",
    "                err_model = err_model + err_mod\n",
    "\n",
    "            # Counter reset\n",
    "            cntr_batch = 0\n",
    "\n",
    "            print('\\n*******************************************************************************')\n",
    "            print('***** Currently at ', int(cntr/tot_samples*100), '%')\n",
    "            print('***** Pseudo_labels: ', pseudo_labels.astype(int))\n",
    "            print('***** Predictions:   ', predictions.astype(int))\n",
    "            print('***** True labels:   ', label_batch)\n",
    "            print('\\n*******************************************************************************\\n')      \n",
    "            \n",
    "        # Counters Update\n",
    "        cntr_batch +=1\n",
    "        cntr += 1 \n",
    "    \n",
    "    # Condition for exiting the loop at end training\n",
    "    if (cntr == tot_samples + 1):\n",
    "        sp.write(b\"endt\")\n",
    "        myClass.TRAINING_FLAG = 5\n",
    "        myClass.cont += 1\n",
    "    \n",
    "    if (myClass.cont == 1):\n",
    "        break\n",
    "\n",
    "print('\\n*******************************************************************************')\n",
    "print('***** Percentual error clustering committed ', int(err_cluster/testing_number*100),'%')\n",
    "print('\\n*******************************************************************************\\n')\n",
    "\n",
    "print('\\n*******************************************************************************')\n",
    "print('***** Percentual error model committed ', int(err_model/testing_number*100),'%')\n",
    "print('\\n*******************************************************************************\\n')\n",
    "\n",
    "\n",
    "# This is section is for generating new features                                           #\n",
    "# The vector of the current feature is saved in a vector that contains, at the end of the  #\n",
    "# all the fetures of all digits.                                                           #\n",
    "\n",
    "#if z == 0:\n",
    "#    feature_file = np.copy(feature_file_0)\n",
    "#    label_file = np.copy(label_file_0)\n",
    "#else:\n",
    "#    feature_file = np.concatenate((feature_file,feature_file_0))\n",
    "#    label_file = np.append(label_file,label_file_0)\n",
    "\n",
    "print('*******************************************************************************')\n",
    "print('***** The training images are finished, press ANY KEY to close the script *****')\n",
    "print('*******************************************************************************')\n",
    "\n",
    "#np.savetxt('ll_feat_100.txt', feature_file, fmt='%.3f')\n",
    "#np.savetxt('ll_lab_feat_100.txt', label_file.astype(int),fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
