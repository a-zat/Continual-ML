{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP CODE\n",
    "def trainOneEpoch_OL(model, x_train, x_test, y_train, y_test, features_saved, labels_features_saved, batch_size):\n",
    "\n",
    "    # RINOMINATO FEATURES -> SAVED FEATURES\n",
    "       \n",
    "    cntr = 1 #  A COSA SERVE?\n",
    "    learn_rate = model.l_rate\n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    tot_samples = train_samples + test_samples\n",
    "    \n",
    "    # Merge train and test arrays\n",
    "    x_tot = np.concatenate((x_train, x_test))   # Images\n",
    "    y_tot = np.concatenate((y_train, y_test))   # Labels\n",
    "    \n",
    "    n_cluster = 10\n",
    "    err_tot = 0\n",
    "    cntr_clus = 0\n",
    "    reminder = tot_samples%batch_size\n",
    "    max_iter = int(tot_samples//batch_size)\n",
    "\n",
    "    features_images = np.zeros(tot_samples)\n",
    "        \n",
    "    # FEATURE EXTRACTION\n",
    "    # Estrae le features per ciascuna immagine utilizzando il Frozen Model\n",
    "    print('**********************************\\n Performing features extraction \\n')\n",
    "\n",
    "    for i in range(0, tot_samples):\n",
    "        features = model.ML_frozen.predict(x_tot[i].reshape((1,28,28,1)), verbose = False)\n",
    "\n",
    "        if i == 0:\n",
    "          features_images = np.copy(features)       \n",
    "        else:\n",
    "          features_images = np.concatenate((features_images, features))\n",
    "\n",
    "    # CLUSTERING\n",
    "    print('**********************************\\n Performing clustering\\n')\n",
    "\n",
    "    # Pseudo-labels\n",
    "    for i in range(0, max_iter):\n",
    "      \n",
    "        # pseudo_label, err = k_mean_clustering(features_images[i*batch_size:i*batch_size+batch_size], features_saved, y_tot[i*batch_size:i*batch_size+batch_size], labels_features, n_cluster, batch_size)\n",
    "        pseudo_label, err = k_mean_clustering(features_images[i*batch_size:i*batch_size+batch_size], features_saved, y_tot[i*batch_size:i*batch_size+batch_size], labels_features_saved, n_cluster, batch_size)\n",
    "       \n",
    "        err_tot += err\n",
    "        pseudo_label = pseudo_label.astype(int)\n",
    "\n",
    "        if i == 0:\n",
    "          pseudo_labels = np.copy(pseudo_label)      \n",
    "        else:\n",
    "          pseudo_labels = np.append(pseudo_labels, pseudo_label)\n",
    "\n",
    "        #print(f\"\\r    Currently at {np.round(np.round(cntr_clus/tot_samples*batch_size,4)*100,2)}% of dataset\", end=\"\")\n",
    "        #print(\"\\n\")\n",
    "        cntr_clus +=1\n",
    "\n",
    "        # k_mean_clustering(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size):\n",
    "    \n",
    "        print(\"Current batch errors:\", str(err) + \"/\"+ str(batch_size), \"(accuracy \" + str((1 - err/batch_size)*100) +\")%\")\n",
    "\n",
    "\n",
    "    # Pseudo-labels for last batch\n",
    "    \n",
    "    if reminder != 0: \n",
    "        pseudo_label, err = k_mean_clustering(features_images[max_iter*batch_size:tot_samples], features_saved, y_tot[max_iter*batch_size:tot_samples], labels_features_saved, n_cluster, reminder)\n",
    "        err_tot = err_tot + err\n",
    "        pseudo_labels = np.append(pseudo_labels, pseudo_label)\n",
    "\n",
    "        print(\"Current batch errors:\", err)\n",
    "\n",
    "    # Check pseudo-labels and errors in clustering\n",
    "    \n",
    "    #print(\"Pseudo_labels vector: \")\n",
    "    #print(pseudo_labels)\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    print(\"The error in clustering is: \")\n",
    "    print(int(err_tot/tot_samples*100), '%')\n",
    "    print(\"errors:\", err_tot)\n",
    "    print(\"tot samples\", tot_samples)\n",
    "\n",
    "    # ONLINE-LEARNING\n",
    "\n",
    "    print('**********************************\\n Performing training with OL\\n')\n",
    "\n",
    "    for i in range(0, tot_samples):\n",
    "\n",
    "        CheckLabelKnown(model, pseudo_labels[i])\n",
    "    \n",
    "        y_true_soft = DigitToSoftmax(pseudo_labels[i], model.label)\n",
    "               \n",
    "        # Prediction\n",
    "        \n",
    "        y_pred = model.predict(features_images[i,:])\n",
    "        \n",
    "        # Backpropagation\n",
    "        cost = y_pred-y_true_soft\n",
    "        \n",
    "        for j in range(0,model.W.shape[0]):\n",
    "\n",
    "            # Update weights\n",
    "            dW = np.multiply(cost, features_images[i,j]*learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db      = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        # the next part is only to plot the confusion matrix\n",
    "        # if the train data is finished still train the model but do not save the results\n",
    "        if(i>=train_samples):\n",
    "\n",
    "            y_true_soft = DigitToSoftmax(y_tot[i], model.label)\n",
    "                   \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            #for k in range(0,len(model.label)):\n",
    "            #    if(model.label[max_i_pred] == model.std_label[k]):\n",
    "            #        p = np.copy(k)\n",
    "            #    if(model.label[max_i_true] == model.std_label[k]):\n",
    "            #        t = np.copy(k)\n",
    "            #\n",
    "            #model.conf_matr[t,p] += 1  \n",
    "\n",
    "        # ComputeEvalMetrics(y_pred)\n",
    "\n",
    "        #print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        #print(\"\\n\")\n",
    "        #cntr +=1\n",
    "\n",
    "    '''Function to compute kmean clustering on the new dataset and the saved features'''\n",
    "def k_mean_clustering(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size):\n",
    "\n",
    "  # Define initial set of features\n",
    "  labels_init_list = list(range(0, n_cluster))\n",
    "\n",
    "  # labels_init_list = list([1, 9, 5, 0])\n",
    "  # labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "  # n_cluster = len(labels_init_list)\n",
    "\n",
    "  # Extract from the saved features the labels that we need\n",
    "  features_saved_init = []\n",
    "  labels_features_saved_init = []\n",
    "  # Extract features of digits considered in labels_init_list\n",
    "  for i in range(0, len(features_saved)):\n",
    "      if labels_features_saved[i] in labels_init_list:\n",
    "        features_saved_init.append(features_saved[i,:])\n",
    "        labels_features_saved_init.append(labels_features_saved[i])\n",
    "  \n",
    "  # Convert list to nparray\n",
    "  features = np.array(features_saved_init)\n",
    "  features = features.astype('float32')\n",
    "  labels_features = np.array(labels_features_saved_init)  \n",
    "\n",
    "  # Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n",
    "  features = np.concatenate((features, features_run))\n",
    "  labels_features = np.append(labels_features, labels_features_run).astype(int)\n",
    "\n",
    "  # Repeat until clustering is correct\n",
    "  while True:\n",
    "    # KMean Clustering\n",
    "    k_mean = create_k_mean(features, n_cluster)\n",
    "\n",
    "    # Find pseudolabels for each new image\n",
    "    # Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n",
    "    clusters_features_saved = list(k_mean.labels_[0:len(labels_features_saved_init)])\n",
    "    labels_features_saved_init = list(labels_features_saved_init)\n",
    "    cluster_list = list(range(0,n_cluster))\n",
    "    map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_features_saved, labels_features_saved_init, cluster_list, labels_init_list) \n",
    " \n",
    "    if len(map_clu2lbl) == n_cluster:\n",
    "        # Exit the loop\n",
    "        break\n",
    "\n",
    "  clusters_features = k_mean.labels_\n",
    "\n",
    "  # Find pseudo label (labels obtained from the model of each image\n",
    "  pseudolabels_features = np.zeros(len(clusters_features), dtype=int)\n",
    "\n",
    "  #print(\"cluster features\", clusters_features)\n",
    "  #print(\"labels features\", labels_features)\n",
    "\n",
    "  # Compute pseudolabels only for new digits\n",
    "  err = 0 # Initialize error counter\n",
    "  for i in range(len(clusters_features) - batch_size, len(clusters_features)):\n",
    "    pseudolabels_features[i] = map_clu2lbl[clusters_features[i]]\n",
    "\n",
    "    # print(pseudolabels_features[i], labels_features[i], clusters_features[i])\n",
    "    if pseudolabels_features[i] != labels_features[i]:\n",
    "      err += 1\n",
    "  \n",
    "  # print(pseudolabels_features)\n",
    "  # print(labels_features[len(clusters_features)-batch_size:len(clusters_features)])\n",
    "\n",
    "  # conf_matrix(clusters_features_saved, labels_features_saved_init, , labels_init_list)\n",
    "  \n",
    "  # Evaluation metrics\n",
    "  # ComputeEvalMetrics(labels_features, pseudolabels_features, cluster_list, labels_init_list)\n",
    "\n",
    "  return pseudolabels_features, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New code - need fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE --- NEED TESTS\n",
    "'''Function to compute kmean clustering on the new dataset and the saved features'''\n",
    "def k_mean_clustering(features_run, labels_run, features_saved, labels_saved, n_cluster, batch_size):\n",
    "  \n",
    "  # Convert list to nparray\n",
    "  #features = np.array(features_saved)\n",
    "  # features = features.astype('float32')\n",
    "  #labels = np.array(labels_saved)  \n",
    "\n",
    "  labels_init_list = list(range(0, n_cluster)) # TEMPORARY\n",
    "\n",
    "  # Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n",
    "  features = np.concatenate((features_saved, features_run))\n",
    "  labels = np.append(labels_saved, labels_run).astype(int)\n",
    "\n",
    "  # Repeat until clustering is correct\n",
    "  max_iter = 20\n",
    "  iter = 0\n",
    "  while True:\n",
    "    # KMean Clustering\n",
    "    k_mean = create_k_mean(features, n_cluster)\n",
    "\n",
    "    # Find pseudolabels for each new image\n",
    "    # Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n",
    "    clusters_saved = list(k_mean.labels_[0:len(labels_saved)])\n",
    "    labels_saved = list(labels_saved)\n",
    "    cluster_list = list(range(0,n_cluster))\n",
    "    map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_saved, labels_saved, cluster_list, labels_init_list) \n",
    " \n",
    "    if len(map_clu2lbl) == n_cluster or iter > max_iter:\n",
    "        if(iter > max_iter):\n",
    "          print(\"Clustering did not converge. Skipping batch.NOT WORKING ---- NEED FIX-----\")\n",
    "        # Exit the loop\n",
    "        break\n",
    "\n",
    "  clusters_features = k_mean.labels_\n",
    "\n",
    "  # Find pseudo label (labels obtained from the model of each image\n",
    "  pseudolabels = np.zeros(len(clusters_features), dtype=int)\n",
    "\n",
    "  # Compute pseudolabels only for new digits\n",
    "  err = 0 # Initialize error counter\n",
    "  for i in range(len(clusters_features) - batch_size, len(clusters_features)):\n",
    "    pseudolabels[i] = map_clu2lbl[clusters_features[i]]\n",
    "\n",
    "    #print(pseudolabels[i], labels[i], clusters_features[i])\n",
    "    if pseudolabels[i] != labels[i]:\n",
    "      err += 1\n",
    "      print(\"label:\",labels[i] ,\"pseudolabel:\", pseudolabels[i])\n",
    "  \n",
    "  # Evaluation metrics\n",
    "  # ComputeEvalMetrics(labels_features, pseudolabels_features, labels_init_list)\n",
    "\n",
    "\n",
    "  return pseudolabels, err\n",
    "\n",
    "''' Function to check if the current label is already known to the model. If not it augments the custom layer adding a new node'''\n",
    "def CheckLabelKnown(model, current_label):\n",
    "    \n",
    "    found = False\n",
    "    \n",
    "    for i in range(0, len(model.label)):\n",
    "        if(current_label == model.label[i]):\n",
    "            found = True\n",
    "          \n",
    "    # If the label is not known\n",
    "    if not found:\n",
    "       #  print(f'\\n\\n    New digit detected -> digit \\033[1m{current_label}\\033[0m \\n')\n",
    "        print(\"new digit detected\", current_label )\n",
    "\n",
    "        model.label.append(current_label)   # Add new digit to label\n",
    "                \n",
    "        # Increase weights and biases dimensions\n",
    "        model.W = np.hstack((model.W, np.zeros([model.W.shape[0],1])))\n",
    "        model.b = np.hstack((model.b, np.zeros([1])))\n",
    "        \n",
    "        model.W_2 = np.hstack((model.W_2, np.zeros([model.W.shape[0],1])))\n",
    "        model.b_2 = np.hstack((model.b_2, np.zeros([1])))\n",
    "\n",
    "\n",
    "def update_active_layer(model, features, pseudo_label):\n",
    "\n",
    "    learn_rate = model.l_rate\n",
    "\n",
    "    CheckLabelKnown(model, pseudo_label)\n",
    "    \n",
    "    y_true_soft = DigitToSoftmax(pseudo_label, model.label)\n",
    "               \n",
    "    # Prediction\n",
    "    y_pred = model.predict(features)\n",
    "        \n",
    "    # Backpropagation\n",
    "    cost = y_pred-y_true_soft\n",
    "        \n",
    "    for j in range(0,model.W.shape[0]):\n",
    "\n",
    "         # Update weights\n",
    "        dW = np.multiply(cost, features[j]*learn_rate)\n",
    "        model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "    # Update biases\n",
    "    db      = np.multiply(cost, learn_rate)\n",
    "    model.b = model.b-db\n",
    "           \n",
    "    # the next part is only to plot the confusion matrix\n",
    "    # if the train data is finished still train the model but do not save the results\n",
    "    #if(i>=train_samples):\n",
    "\n",
    "    #    y_true_soft = DigitToSoftmax(y_tot[i], model.label)\n",
    "                   \n",
    "        # Find the max iter for both true label and prediction\n",
    "    #    if(np.amax(y_true_soft) != 0):\n",
    "    #        max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "    #    if(np.amax(y_pred) != 0):\n",
    "    #        max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainOneEpoch_OL(model, images, labels, features_saved, labels_saved, batch_size):\n",
    "    \n",
    "    n_cluster = 10\n",
    "    n_samples = images.shape[0]\n",
    "\n",
    "    # Features extraction\n",
    "    # features = model.ML_frozen.predict(images.reshape((1,28,28,1)), verbose = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define initial set of features\n",
    "    labels_init_list = list(range(0, n_cluster))\n",
    "\n",
    "    # labels_init_list = list([1, 9, 5, 0])\n",
    "    # labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    # n_cluster = len(labels_init_list)\n",
    "\n",
    "    # Extract from the saved features the labels that we need\n",
    "    features_saved_sel = []\n",
    "    labels_saved_sel = []\n",
    "    # Extract features of digits considered in labels_init_list\n",
    "    for i in range(0, len(features_saved)):\n",
    "        if labels_features_saved[i] in labels_init_list:\n",
    "            features_saved_sel.append(features_saved[i,:])\n",
    "            labels_saved_sel.append(labels_saved[i])\n",
    "\n",
    "\n",
    "    # Split dataset in batches\n",
    "    n_batch = int(np.ceil(n_samples / batch_size))\n",
    "    images_batch = np.array_split(images, n_batch)\n",
    "    labels_batch = np.array_split(labels, n_batch)\n",
    "\n",
    "\n",
    "    err_tot = 0\n",
    "    pseudo_labels = np.empty\n",
    "    for i in range(0, n_batch):\n",
    "        print(\"Starting batch:\" + str(i) + \"/\" + str(n_batch))\n",
    "        # Features extraction\n",
    "        start1 = time.time()\n",
    "        features = model.ML_frozen.predict(images_batch[i].reshape((batch_size,28,28,1)), verbose = False) # VERIFICO\n",
    "        end1 = time.time()\n",
    "\n",
    "        # Kmean clustering\n",
    "        start2 = time.time()\n",
    "        pseudo_label, err = k_mean_clustering(features, labels_batch, features_saved_sel, labels_saved_sel, n_cluster, batch_size)\n",
    "        end2 = time.time()\n",
    "        pseudo_labels = np.append(pseudo_labels, pseudo_label.astype(int))\n",
    "        err_tot += err\n",
    "\n",
    "        print(\"Features extraction took {:f} seconds\".format(end1 - start1), \"and Kmean clustering took {:f} seconds\".format(end2 - start2), \", with {}\".format((1-err/features.shape[0])*100), \"%\", \"accuracy\", \"({} errors)\".format(err))\n",
    "\n",
    "    print(\"The error in clustering is: {}\".format(int(err_tot/n_samples*100)), '%')\n",
    "    print(\"Errors:\", err_tot, \"Samples: \", n_samples)\n",
    "\n",
    "    # ONLINE-LEARNING\n",
    "\n",
    "    print('**********************************\\n Performing training with OL\\n')\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "        pass\n",
    "        # update_active_layer(model, features[i,:], pseudo_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "digits = np.concatenate((digits_train, digits_test))   \n",
    "labels = np.concatenate((label_digits_train, label_digits_test))  \n",
    "trainOneEpoch_OL(Model_OL, digits, labels, features_saved, labels_features_saved, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
