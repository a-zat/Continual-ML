{"cells":[{"cell_type":"markdown","metadata":{"id":"D8g3_OkUNOuD"},"source":["## **Import the TensorFlow library**"]},{"cell_type":"markdown","metadata":{"id":"UKk-D3IZkkbE"},"source":["This code contains the training for a CNN in which the model learns to recognize the digits. This is different from the other training script because the model learns the digits from 0 to 6. The Idea is to have the digits 7,8,9 for the OL learning to be applied on STM32 Nucleo Board."]},{"cell_type":"markdown","metadata":{"id":"3niaTGoZdbpB"},"source":["Questa parte di codice serve solo per essere compatibile con la OpenMV Cam"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":5736,"status":"ok","timestamp":1665998604312,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"XCqcQuaBLNgF"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Dropout, Dense, Flatten, Reshape\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import backend as K\n","import matplotlib.pyplot as plt \n","import numpy as np\n","import random\n","import os\n","import csv \n","import tempfile\n","\n","# Absolute path is needed to load libraries \n","import sys\n","ROOT_PATH = os.path.abspath('')\n","sys.path.append(ROOT_PATH + '/lib')\n","\n","from lib.FrozenNN_lib import *\n","from lib.PlotUtils import *"]},{"cell_type":"markdown","metadata":{"id":"VT8C9aeAMdSE"},"source":["Load MNIST dataset and split in training and test"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1665998604945,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"mNfeJ2bbNDET","outputId":"08220356-e5f0-4c96-e364-9ca188f39331"},"outputs":[{"name":"stdout","output_type":"stream","text":["The original dataset shapes are\n","    Train dataset shape: (60000, 28, 28)\n","    Test dataset shape:  (10000, 28, 28)\n"]}],"source":["(data_train, label_train),(data_test, label_test) = mnist.load_data() # Load data\n","print('The original dataset shapes are')\n","print(f'    Train dataset shape: {data_train.shape}')\n","print(f'    Test dataset shape:  {data_test.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["# Options"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Define High and Low sets\n","low_set = set(range(0,6))\n","high_set = set(range(6,10))\n","\n","# test elements\n","n_elem_low = 1000   # Number of elements for each label to add in data_low_test\n","n_elem_high = 1000  # Number of elements for each label to add in data_high_test\n","\n","#Â Training\n","epochs =60\n","batch_size = 32\n","\n","## Save path\n","if(n_elem_low == n_elem_high):\n","  n_elem = str(n_elem_low)\n","else:\n","  n_elem = str(n_elem_low) + '_' + str(n_elem_high)\n","\n","save = True\n","SAVE_MODEL_PATH = ROOT_PATH + \"/Models/\" + str(n_elem) + \"/\""]},{"cell_type":"markdown","metadata":{"id":"LaAIs1HlrltM"},"source":["Split the train and test dataset in smaller datasets. Separation criterion is if the number is smaller than 6 or not."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["After the separation of the dataset in groups higer and lower/equal than 6 the datasets are:\n","     Train dataset lower than 6 has shape:  (36017, 28, 28)\n","     Train dataset higher than 6 has shape: (23983, 28, 28)\n","\n","     Test dataset lower than 6 has shape:  (6000, 28, 28)\n","     Test dataset higher than 6 has shape: (4000, 28, 28)\n"]}],"source":["# New separation script\n","train_samples = label_train.shape[0]\n","test_samples  = label_test.shape[0]\n","img_rows, img_cols = 28, 28\n","\n","trainLow_samples = 0\n","testLow_samples = 0\n","for lbl in low_set:\n","    trainLow_samples += (label_train == lbl).sum()\n","    testLow_samples += (label_test == lbl).sum()\n","\n","# SPLIT TRAIN DATA\n","# Split train dataset in high and low\n","data_low_train   = np.zeros([trainLow_samples,28,28])\n","label_low_train  = np.zeros(trainLow_samples)\n","data_high_train  = np.zeros([train_samples-trainLow_samples,28,28])\n","label_high_train = np.zeros(train_samples-trainLow_samples)\n","\n","j,k = 0,0\n","for i in range(0,train_samples):  \n","    if(label_train[i]<6):\n","        data_low_train[j,:,:] = data_train[i,:,:]\n","        label_low_train[j]    = label_train[i]\n","        j+=1\n","    else:\n","        data_high_train[k,:,:] = data_train[i,:,:]\n","        label_high_train[k]    = label_train[i]\n","        k+=1\n","\n","# SPLIT TEST DATA\n","# Split test dataset in high and low. Number of testing elements is predefined\n","n_low = n_elem_low * len(low_set)\n","n_high = n_elem_high * len(high_set)\n","\n","data_low_test   = np.zeros([n_low,28,28])\n","label_low_test  = np.zeros(n_low)\n","data_high_test  = np.zeros([n_high,28,28])\n","label_high_test = np.zeros(n_high)\n","\n","digits_set = low_set.union(high_set)\n","counter = {x: 0 for x in digits_set}\n","\n","j,k = 0,0\n","for i in range(0,test_samples):  \n","    if(label_test[i] in low_set):\n","        if(counter[label_test[i]] < n_elem_low):\n","            data_low_test[j,:,:] = data_test[i,:,:]\n","            label_low_test[j]    = label_test[i]\n","            counter[label_test[i]] += 1\n","            j += 1\n","    else:\n","        if(counter[label_test[i]] < n_elem_high):\n","            data_high_test[k,:,:] = data_test[i,:,:]\n","            label_high_test[k]    = label_test[i]\n","            counter[label_test[i]] += 1\n","            k += 1   \n","\n","print('After the separation of the dataset in groups higer and lower/equal than 6 the datasets are:')\n","print(f'     Train dataset lower than 6 has shape:  {data_low_train.shape}')\n","print(f'     Train dataset higher than 6 has shape: {data_high_train.shape}')\n","print()\n","print(f'     Test dataset lower than 6 has shape:  {data_low_test.shape}')\n","print(f'     Test dataset higher than 6 has shape: {data_high_test.shape}')"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1665998606607,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"bgVHZlEqqBr7","outputId":"2e943778-c123-485f-ac5e-f0962afac393"},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n","7.0\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Randomly check if dataset that I created are filled correctly\n","num = int(random.uniform(0,20))\n","print(num)\n","plt.imshow(data_high_test[num], cmap=\"gray\") # Import the image\n","print(label_high_test[num])\n","plt.show() # Plot the image"]},{"cell_type":"markdown","metadata":{"id":"MTx7YrtENh3F"},"source":["## **Pre process the data**"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1665998606608,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"c2TsbEYpU-p2"},"outputs":[],"source":["# Something I don't know\n","if K.image_data_format() == 'channels_first':\n","    data_low_train  = data_low_train.reshape(data_low_train.shape[0], 1, img_rows, img_cols)\n","    data_high_train = data_high_train.reshape(data_high_train.shape[0], 1, img_rows, img_cols)\n","    data_low_test   = data_low_test.reshape(data_low_test.shape[0], 1, img_rows, img_cols)\n","    data_high_test  = data_high_test.reshape(data_high_test.shape[0], 1, img_rows, img_cols)\n","    input_shape     = (1, img_rows, img_cols)\n","else:\n","    data_low_train  = data_low_train.reshape(data_low_train.shape[0], img_rows, img_cols, 1)\n","    data_high_train = data_high_train.reshape(data_high_train.shape[0], img_rows, img_cols, 1)\n","    data_low_test   = data_low_test.reshape(data_low_test.shape[0], img_rows, img_cols, 1)\n","    data_high_test  = data_high_test.reshape(data_high_test.shape[0], img_rows, img_cols, 1)\n","    input_shape     = (img_rows, img_cols, 1)\n","\n","# Normalize the colors from 0-255 to 0-1\n","data_low_train  = data_low_train.astype(np.float32) / 255.0\n","data_high_train = data_high_train.astype(np.float32) / 255.0\n","data_low_test   = data_low_test.astype(np.float32) / 255.0\n","data_high_test  = data_high_test.astype(np.float32) / 255.0"]},{"cell_type":"markdown","metadata":{"id":"TNaCD_O0RPDs"},"source":["## **BUILD THE MODEL**"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665998606609,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"dewlaeUUlFpw"},"outputs":[],"source":["# Model selection\n","TRAIN_MODEL_1 = True\n","TRAIN_MODEL_2 = False\n","TRAIN_MODEL_3 = False\n","\n","# Training options\n","batch_size = batch_size\n","epochs     = epochs\n","validation_split = 0.1\n","optimizer  = \"adam\"\n","loss       = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics    = ['accuracy']"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665998606609,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"MkaZwkbYmj4s"},"outputs":[],"source":["if(False):\n","    model = Sequential()\n","    model.add(Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(6, activation = \"softmax\"))\n","\n","    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n","    model.summary()"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2175,"status":"ok","timestamp":1665998608776,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"vZVNdEatp8L2","outputId":"07100436-8659-4bfc-8245-7423327141b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_8 (Conv2D)           (None, 26, 26, 8)         80        \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 24, 24, 8)         584       \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 12, 12, 8)        0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 10, 10, 32)        2336      \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 8, 8, 32)          9248      \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 4, 4, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 4, 4, 32)          0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 512)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 6)                 3078      \n","                                                                 \n","=================================================================\n","Total params: 15,326\n","Trainable params: 15,326\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["if(TRAIN_MODEL_1):\n","    model = Sequential()\n","    \n","    model.add(Conv2D(8, kernel_size=(3,3), activation='relu',input_shape=input_shape))\n","    model.add(Conv2D(8, (3,3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","    \n","    model.add(Conv2D(32, (3,3), activation='relu'))\n","    model.add(Conv2D(32, (3,3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","\n","    model.add(Dropout(0.25))\n","    model.add(Flatten())\n","    model.add(Dense(6,activation='softmax'))\n","\n","    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","    print(model.summary())"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665998608777,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"EhPlI-Ojmj4u"},"outputs":[],"source":["#if(TRAIN_MODEL_1):\n","    #tf.keras.utils.plot_model(model, show_shapes=True, to_file='naive_inception_module.png')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665998608778,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"Cige2fQHFXGB"},"outputs":[],"source":["# METHOD 2\n","# This model is a bit larger and should be much more precise in the feature extraction\n","if(TRAIN_MODEL_2):\n","    model2 = Sequential()\n","    model2.add(Conv2D(32, (3, 3), input_shape = input_shape))\n","    model2.add(Conv2D(32, (3, 3), activation = \"relu\"))\n","    model2.add(MaxPooling2D(pool_size = (2, 2)))\n","    model2.add(Dropout(0.2))\n","    model2.add(Flatten())\n","    model2.add(Dense(128, activation = \"relu\"))\n","    model2.add(Dropout(0.2))\n","    model2.add(Dense(6, activation = \"softmax\"))\n","\n","    model2.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n","    model2.summary()"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1665998608778,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"pxMSgJ2ymj4v"},"outputs":[],"source":["if(TRAIN_MODEL_3):\n","\n","    l = tf.keras.layers # syntax shortcut\n","\n","    def fire(x, squeeze, expand):\n","        y = l.Conv2D(filters=squeeze, kernel_size=1, padding='same', activation='relu')(x)\n","        y1 = l.Conv2D(filters=expand//2, kernel_size=1, padding='same', activation='relu')(y)\n","        y3 = l.Conv2D(filters=expand//2, kernel_size=3, padding='same', activation='relu')(y)\n","        return tf.keras.layers.concatenate([y1, y3])\n","\n","    # this is to make it behave similarly to other Keras layers\n","    def fire_module(squeeze, expand):\n","        return lambda x: fire(x, squeeze, expand)\n","\n","    # usage:\n","    x = tf.keras.layers.Input(shape=[*input_shape]) # input is 192x192 pixels RGB\n","\n","    y = tf.keras.layers.Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(x)\n","    y = fire_module(24, 48)(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n","    y = fire_module(24, 48)(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n","    y = fire_module(24, 48)(y)\n","    y = tf.keras.layers.GlobalAveragePooling2D()(y)\n","    y = tf.keras.layers.Dense(6, activation='softmax')(y)\n","\n","    model3 = tf.keras.Model(x, y)\n","    model3.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n","    model3.summary()"]},{"cell_type":"markdown","metadata":{"id":"P_3CoNfUFXGE"},"source":["## TRAIN THE MODEL"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214784,"status":"ok","timestamp":1665998823555,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"ZRk1oPJmTCM2","outputId":"5e03bbf3-013c-497a-814f-8b2c5e1bc26b","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n"]},{"name":"stderr","output_type":"stream","text":["/Users/andrea/Documents/VS Code/MachineLearning/venv/lib/python3.10/site-packages/keras/backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["1013/1013 [==============================] - 14s 13ms/step - loss: 0.1568 - accuracy: 0.9491 - val_loss: 0.0346 - val_accuracy: 0.9892\n","Epoch 2/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.0287 - val_accuracy: 0.9919\n","Epoch 3/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0299 - accuracy: 0.9906 - val_loss: 0.0237 - val_accuracy: 0.9936\n","Epoch 4/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0236 - val_accuracy: 0.9950\n","Epoch 5/60\n","1013/1013 [==============================] - 14s 14ms/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 0.0183 - val_accuracy: 0.9956\n","Epoch 6/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.0200 - val_accuracy: 0.9958\n","Epoch 7/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0216 - val_accuracy: 0.9961\n","Epoch 8/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0242 - val_accuracy: 0.9947\n","Epoch 9/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0195 - val_accuracy: 0.9961\n","Epoch 10/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0244 - val_accuracy: 0.9956\n","Epoch 11/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0177 - val_accuracy: 0.9958\n","Epoch 12/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.0258 - val_accuracy: 0.9947\n","Epoch 13/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0216 - val_accuracy: 0.9964\n","Epoch 14/60\n","1013/1013 [==============================] - 38s 38ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0246 - val_accuracy: 0.9950\n","Epoch 15/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0228 - val_accuracy: 0.9967\n","Epoch 16/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0180 - val_accuracy: 0.9967\n","Epoch 17/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0277 - val_accuracy: 0.9961\n","Epoch 18/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0210 - val_accuracy: 0.9969\n","Epoch 19/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0268 - val_accuracy: 0.9956\n","Epoch 20/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 0.0197 - val_accuracy: 0.9967\n","Epoch 21/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0264 - val_accuracy: 0.9961\n","Epoch 22/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0243 - val_accuracy: 0.9969\n","Epoch 23/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0220 - val_accuracy: 0.9967\n","Epoch 24/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0225 - val_accuracy: 0.9981\n","Epoch 25/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0239 - val_accuracy: 0.9967\n","Epoch 26/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0326 - val_accuracy: 0.9958\n","Epoch 27/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0256 - val_accuracy: 0.9975\n","Epoch 28/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0262 - val_accuracy: 0.9972\n","Epoch 29/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0217 - val_accuracy: 0.9969\n","Epoch 30/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0235 - val_accuracy: 0.9969\n","Epoch 31/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0245 - val_accuracy: 0.9967\n","Epoch 32/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0272 - val_accuracy: 0.9964\n","Epoch 33/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0264 - val_accuracy: 0.9964\n","Epoch 34/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0314 - val_accuracy: 0.9964\n","Epoch 35/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0277 - val_accuracy: 0.9969\n","Epoch 36/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0274 - val_accuracy: 0.9975\n","Epoch 37/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0264 - val_accuracy: 0.9964\n","Epoch 38/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0327 - val_accuracy: 0.9975\n","Epoch 39/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0279 - val_accuracy: 0.9964\n","Epoch 40/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0290 - val_accuracy: 0.9967\n","Epoch 41/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0369 - val_accuracy: 0.9964\n","Epoch 42/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0295 - val_accuracy: 0.9961\n","Epoch 43/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0228 - val_accuracy: 0.9972\n","Epoch 44/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0237 - val_accuracy: 0.9975\n","Epoch 45/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0245 - val_accuracy: 0.9975\n","Epoch 46/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.0257 - val_accuracy: 0.9975\n","Epoch 47/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0341 - val_accuracy: 0.9969\n","Epoch 48/60\n","1013/1013 [==============================] - 12s 11ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0297 - val_accuracy: 0.9958\n","Epoch 49/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0295 - val_accuracy: 0.9967\n","Epoch 50/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0304 - val_accuracy: 0.9958\n","Epoch 51/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0269 - val_accuracy: 0.9967\n","Epoch 52/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0240 - val_accuracy: 0.9969\n","Epoch 53/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0256 - val_accuracy: 0.9972\n","Epoch 54/60\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0249 - val_accuracy: 0.9961\n","Epoch 55/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0362 - val_accuracy: 0.9967\n","Epoch 56/60\n","1013/1013 [==============================] - 11s 11ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0353 - val_accuracy: 0.9969\n","Epoch 57/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0312 - val_accuracy: 0.9967\n","Epoch 58/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0298 - val_accuracy: 0.9975\n","Epoch 59/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0291 - val_accuracy: 0.9972\n","Epoch 60/60\n","1013/1013 [==============================] - 12s 12ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0439 - val_accuracy: 0.9950\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2958 - accuracy: 0.9730\n","\n","Test accuracy: 0.9729999899864197\n"]}],"source":["if(TRAIN_MODEL_1):\n","    \n","    labels_prova = keras.utils.to_categorical(label_low_train, 6)\n","\n","    train_hist = model.fit(data_low_train, label_low_train, epochs = epochs, batch_size = batch_size, validation_split = validation_split)\n","    \n","  # Evaluate the model performance\n","    test_loss, test_acc = model.evaluate(data_low_test, label_low_test)\n","\n","    print('\\nTest accuracy:', test_acc)   # Print out the model accuracy "]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1665998823555,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"h_oS2hh3FXGO","scrolled":false},"outputs":[],"source":["if(TRAIN_MODEL_2):\n","\n","    labels_prova = keras.utils.to_categorical(label_low_train, 6)\n","\n","    model2.fit(data_low_train, labels_prova, epochs = epochs, batch_size = batch_size, validation_split = validation_split )\n","\n","    # Evaluate the model performance\n","    test_loss, test_acc = model2.evaluate(data_low_test, keras.utils.to_categorical(label_low_test, 6))\n","\n","    print('\\nTest accuracy:', test_acc)   # Print out the model accuracy "]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1665998823556,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"ljevV8Snmj4x"},"outputs":[],"source":["if(TRAIN_MODEL_3):\n","    \n","    labels_modified_test = keras.utils.to_categorical(label_low_train, 6)\n","    \n","    model3.fit(data_low_train, labels_modified_test, epochs = epochs, batch_size = batch_size, validation_split = validation_split)\n","\n","    # Evaluate the model performance\n","    test_loss, test_acc = model3.evaluate(data_low_test, keras.utils.to_categorical(label_low_test, 6))\n","\n","    print('\\nTest accuracy:', test_acc)   # Print out the model accuracy "]},{"cell_type":"markdown","metadata":{"id":"tI3aR1l4pqhS"},"source":["## TEST THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"6czdnMPOmj4z"},"source":["function for generating a bar plot htat shows the accuracy of the model for each class"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665998823557,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"UxawR_zdmj41"},"outputs":[],"source":["#confusion_matrix = testing(data_low_test, label_low_test, model)\n","#hostiry_training_plot(train_hist)\n","#plot_Accuracy(confusion_matrix)\n","#plot_ConfusionMatrix(confusion_matrix)\n","#plot_Table(confusion_matrix)"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665998823558,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"AVNdx-gjlhze"},"outputs":[],"source":["if(TRAIN_MODEL_1):\n","    model_test = model\n","elif(TRAIN_MODEL_2):\n","    model_test = model2\n","elif(TRAIN_MODEL_3):\n","    model_test = model3"]},{"cell_type":"markdown","metadata":{"id":"Azibv0yuBtkt"},"source":["Il prossimo blocco non Ã¨ necessario, restituisce errore per le lunghezze diverse di data_low_test e lables_modified_test. Anche mettondole uguali ancora si lamenta."]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665998823558,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"4usCVRWdmj42"},"outputs":[],"source":["# tensorflow evaluation on the test dataset\n","#results = model_test.evaluate(data_low_test,labels_modified_test, batch_size=128)\n","#print(\"test loss, test acc:\", results)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":889,"status":"ok","timestamp":1665998824439,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"piwc0fbspvlc","outputId":"3323284b-2b6a-46d0-86ef-93331db212fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["188/188 [==============================] - 1s 3ms/step\n","Prediction =  5\n","True label =  5.0\n"]}],"source":["predictions = model_test.predict(data_low_test)   # Make prediction of entire dataset\n","\n","num = int(random.uniform(0,data_low_test.shape[0]))\n","print(\"Prediction = \" , np.argmax(predictions[num]))\n","print(\"True label = \" , label_low_test[num])"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1665998824441,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"_L_MP6pejCGk","outputId":"d57c00f9-084b-4d17-91bc-4088144af115"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 600x300 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["num = int(random.uniform(0, predictions.shape[0]))\n","\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","\n","plot_image(num, predictions[num], label_low_test, data_low_test)\n","plt.subplot(1,2,2)\n","\n","plot_value_array(num, predictions[num], label_low_test)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ye7xyl_-vkkk"},"source":["## **SAVE ORIGINAL MODEL AND FROZEN MODEL**"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1604,"status":"ok","timestamp":1665998826031,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"rjFdtesdvqzf","outputId":"e4805664-ade9-4882-b44d-d394ce4f8c10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Save ORIGINAL MODEL as original_mnist_cnn.h5\n"]}],"source":["# print('Test loss:', test_loss)\n","# print('Test accuracy:', test_acc)\n","\n","if save:\n","    print('Save ORIGINAL MODEL as original_mnist_cnn.h5')\n","    model_test.save(SAVE_MODEL_PATH + \"original_mnist_cnn.h5\")\n","    save_summary_model(model_test, SAVE_MODEL_PATH, 0, \"original\")"]},{"cell_type":"markdown","metadata":{"id":"1nPOwtTnFXGk"},"source":["Now create the frozen model version for the one just saved. Which means just to cut away the last layer and save the model without the kast kayer. This should be instead saved in a txt/library file."]},{"cell_type":"markdown","metadata":{"id":"Rk84TZDvyjW4"},"source":["### SAVE THE FROZEN MODEL"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2615,"status":"ok","timestamp":1665998828642,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"qQzEf-OKFXGl","outputId":"b2705bd2-5e13-4f3b-ab10-380814467e1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_8 (Conv2D)           (None, 26, 26, 8)         80        \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 24, 24, 8)         584       \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 12, 12, 8)        0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 10, 10, 32)        2336      \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 8, 8, 32)          9248      \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 4, 4, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 4, 4, 32)          0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 512)               0         \n","                                                                 \n","=================================================================\n","Total params: 12,248\n","Trainable params: 12,248\n","Non-trainable params: 0\n","_________________________________________________________________\n","Save FROZEN MODEL model as frozen_mnist_cnn.h5\n"]}],"source":["# CREATE AND SAVE THE FROZEN MODEL\n","frozen_model = keras.models.Sequential(model_test.layers[:-1])\n","frozen_model.summary()\n","frozen_model.compile()\n","\n","if save:\n","    print('Save FROZEN MODEL model as frozen_mnist_cnn.h5')\n","    frozen_model.save(SAVE_MODEL_PATH + \"frozen_mnist_cnn.h5\")\n","    save_summary_model(frozen_model, SAVE_MODEL_PATH, 1, \"frozen\")"]},{"cell_type":"markdown","metadata":{},"source":["### SAVE LL WEIGHTS (Last Layer)"]},{"cell_type":"markdown","metadata":{"id":"f1wsEHInFXGn"},"source":["Now save also the last layer weights in a txt file, this will later be used in the OpenMV camera for loading the weights that have been already trained. (ll = last layer)"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1665998830721,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"NOvcRsOVFXGn","outputId":"56870492-0e67-4c4c-8500-4fe73aec7af6"},"outputs":[{"name":"stdout","output_type":"stream","text":["The shape of the last layer weights is: (512, 6)\n","The shape of the last layer biases is: (6,)\n"]}],"source":["if save:\n","    ll_weights = np.array(model_test.layers[-1].get_weights()[0])   # get last layer weights from TF model\n","    ll_biases  = np.array(model_test.layers[-1].get_weights()[1])   # get last layer biases from TF model\n","    print(f'The shape of the last layer weights is: {ll_weights.shape}')\n","    print(f'The shape of the last layer biases is: {ll_biases.shape}')\n","\n","\n","    # -------- WEIGHTS\n","    # NB: the filof weights is separated in smaller rows (338 float values on each row)\n","    # thjis is done in order to make it easier for the OpenMV camera to load each line (memory problems)\n","    with open(SAVE_MODEL_PATH + 'll_weights.txt', 'w') as new_file:\n","\n","        for j in range(0, ll_weights.shape[1]):\n","            for i in range(0, ll_weights.shape[0]): \n","                if(i%338==0 and i!= 0 and i != ll_weights.shape[0]-1):\n","                    new_file.write('\\n')\n","                    \n","                new_file.write(str(ll_weights[i,j]))\n","                \n","                if(i == ll_weights.shape[0]-1):\n","                    new_file.write('\\n')\n","                elif((i+1)%338 == 0):\n","                    dummy = 0\n","                else:\n","                    new_file.write(',')\n","\n","    new_file.close()\n","\n","\n","    # -------- BIASES\n","    with open(SAVE_MODEL_PATH + 'll_biases.txt', 'w') as new_file:\n","        for i in range(0, ll_biases.shape[0]):     \n","            new_file.write(str(ll_biases[i])) \n","            if(i!=ll_biases.shape[0]-1):\n","                new_file.write(',')\n","    new_file.close()"]},{"cell_type":"markdown","metadata":{},"source":["### SAVE FEATURES"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["data_test = np.concatenate((data_low_test, data_high_test))\n","label_test = np.concatenate((label_low_test, label_high_test))\n","\n","# Compute predictions\n","features = frozen_model.predict(data_test, verbose = False)\n","\n","if save:\n","  np.savetxt(SAVE_MODEL_PATH + 'll_features.txt', features, fmt='%.3f')\n","\n","  with open(SAVE_MODEL_PATH + 'll_features.txt', 'w') as new_file:\n","      for i in range(0, features.shape[0]):\n","          for j in range(0, features.shape[1]): \n","              if features[i,j] != 0:\n","                str1 = '%.3f '%features[i,j]\n","              else:\n","                str1 = '0 '\n","\n","              new_file.write(str1)           \n","          new_file.write('\\n')     \n","  new_file.close()\n","\n","  np.savetxt(SAVE_MODEL_PATH + 'll_labels_features.txt',label_test, fmt='%1d')"]},{"cell_type":"markdown","metadata":{},"source":["# Pruning"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","import tensorflow_model_optimization as tfmot"]},{"cell_type":"markdown","metadata":{},"source":["Wrap the tf.keras model with pruning functionality which sparsifies the layer's weights during training."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," prune_low_magnitude_conv2d_  (None, 26, 26, 8)        154       \n"," 4 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 24, 24, 8)        1162      \n"," 5 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_max_poo  (None, 12, 12, 8)        1         \n"," ling2d_2 (PruneLowMagnitude                                     \n"," )                                                               \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 10, 10, 32)       4642      \n"," 6 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 8, 8, 32)         18466     \n"," 7 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_max_poo  (None, 4, 4, 32)         1         \n"," ling2d_3 (PruneLowMagnitude                                     \n"," )                                                               \n","                                                                 \n"," prune_low_magnitude_dropout  (None, 4, 4, 32)         1         \n"," _1 (PruneLowMagnitude)                                          \n","                                                                 \n"," prune_low_magnitude_flatten  (None, 512)              1         \n"," _1 (PruneLowMagnitude)                                          \n","                                                                 \n"," prune_low_magnitude_dense_1  (None, 6)                6152      \n","  (PruneLowMagnitude)                                            \n","                                                                 \n","=================================================================\n","Total params: 30,580\n","Trainable params: 15,326\n","Non-trainable params: 15,254\n","_________________________________________________________________\n"]}],"source":["# Compute end step to finish pruning after n epochs.\n","batch_size = 32\n","epochs = 5\n","validation_split = 0.1  # 10% of training set will be used for validation set. \n","\n","num_images = data_low_train.shape[0] * (1 - validation_split)\n","end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n","\n","# Define model for pruning.\n","pruning_params = {\n","    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n","                                                               final_sparsity=0.80,\n","                                                               begin_step=0,\n","                                                               end_step=end_step)\n","}\n","\n","# Wrap tf.keras model with pruning functionality\n","model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model_test, **pruning_params)\n","\n","# Select appropriate optimizer\n","model_for_pruning.compile(optimizer='adam',\n","                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                          metrics=['accuracy'])\n","\n","model_for_pruning.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Train and test the network using the datasets"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/Users/andrea/Documents/VS Code/MachineLearning/venv/lib/python3.10/site-packages/keras/backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["1013/1013 [==============================] - 16s 12ms/step - loss: 0.1572 - accuracy: 0.9493 - val_loss: 0.0516 - val_accuracy: 0.9856\n","Epoch 2/5\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0503 - accuracy: 0.9839 - val_loss: 0.0379 - val_accuracy: 0.9903\n","Epoch 3/5\n","1013/1013 [==============================] - 13s 13ms/step - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.0315 - val_accuracy: 0.9922\n","Epoch 4/5\n","1013/1013 [==============================] - 14s 13ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.0284 - val_accuracy: 0.9947\n","Epoch 5/5\n","1013/1013 [==============================] - 14s 14ms/step - loss: 0.0301 - accuracy: 0.9907 - val_loss: 0.0256 - val_accuracy: 0.9933\n"]},{"ename":"NameError","evalue":"name 'test_acc' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [30], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m model_for_pruning\u001b[39m.\u001b[39mfit(data_low_train, label_low_train,\n\u001b[1;32m      9\u001b[0m                     batch_size\u001b[39m=\u001b[39mbatch_size, epochs\u001b[39m=\u001b[39mepochs, validation_split\u001b[39m=\u001b[39mvalidation_split, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m     11\u001b[0m _, model_for_pruning_accuracy \u001b[39m=\u001b[39m model_for_pruning\u001b[39m.\u001b[39mevaluate(data_low_test, label_low_test, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOriginal test accuracy: \u001b[39m\u001b[39m'\u001b[39m, test_acc)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPruned test accuracy:   \u001b[39m\u001b[39m'\u001b[39m, model_for_pruning_accuracy)\n","\u001b[0;31mNameError\u001b[0m: name 'test_acc' is not defined"]}],"source":["logdir = tempfile.mkdtemp()\n","\n","callbacks = [\n","  tfmot.sparsity.keras.UpdatePruningStep(),\n","  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n","]\n","\n","model_for_pruning.fit(data_low_train, label_low_train,\n","                    batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=callbacks)\n","\n","_, model_for_pruning_accuracy = model_for_pruning.evaluate(data_low_test, label_low_test, verbose=0)\n","\n","print('Original test accuracy: ', test_acc)\n","print('Pruned test accuracy:   ', model_for_pruning_accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["### Save Pruned models (Tensorflow models)\n","Create a compressible model for TensorFlow (.h5)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_4 (Conv2D)           (None, 26, 26, 8)         80        \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 24, 24, 8)         584       \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 12, 12, 8)        0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 10, 10, 32)        2336      \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 8, 8, 32)          9248      \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 4, 4, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 4, 4, 32)          0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 6)                 3078      \n","                                                                 \n","=================================================================\n","Total params: 15,326\n","Trainable params: 15,326\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["PRUNED_MODEL_PATH = SAVE_MODEL_PATH + \"/Pruned_model\"\n","\n","# Save pruned original model\n","model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n","model_for_export.save(PRUNED_MODEL_PATH + '/OMV_Pruned_cnn.h5', include_optimizer=False)\n","model_for_export.summary()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," prune_low_magnitude_conv2d_  (None, 26, 26, 8)        154       \n"," 4 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 24, 24, 8)        1162      \n"," 5 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_max_poo  (None, 12, 12, 8)        1         \n"," ling2d_2 (PruneLowMagnitude                                     \n"," )                                                               \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 10, 10, 32)       4642      \n"," 6 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_conv2d_  (None, 8, 8, 32)         18466     \n"," 7 (PruneLowMagnitude)                                           \n","                                                                 \n"," prune_low_magnitude_max_poo  (None, 4, 4, 32)         1         \n"," ling2d_3 (PruneLowMagnitude                                     \n"," )                                                               \n","                                                                 \n"," prune_low_magnitude_dropout  (None, 4, 4, 32)         1         \n"," _1 (PruneLowMagnitude)                                          \n","                                                                 \n"," prune_low_magnitude_flatten  (None, 512)              1         \n"," _1 (PruneLowMagnitude)                                          \n","                                                                 \n","=================================================================\n","Total params: 24,428\n","Trainable params: 12,248\n","Non-trainable params: 12,180\n","_________________________________________________________________\n","Save FROZEN PRUNED MODEL model as OMV_Frozen_pruned_cnn.h5\n"]}],"source":["# Save pruned frozen model\n","frozen_pruned_model = keras.models.Sequential(model_for_pruning.layers[:-1])\n","frozen_pruned_model.summary()\n","frozen_pruned_model.compile()\n","\n","FROZEN_PRUNED_MODEL_PATH = SAVE_MODEL_PATH + \"/Frozen_Pruned_model\"\n","\n","print('Save FROZEN PRUNED MODEL model as OMV_Frozen_pruned_cnn.h5')\n","frozen_pruned_model.save(FROZEN_PRUNED_MODEL_PATH + \"OMV_Frozen_pruned_cnn.h5\")\n","# save_summary_model(frozen_pruned_model, FROZEN_PRUNED_MODEL_PATH, 1)"]},{"cell_type":"markdown","metadata":{},"source":["### Save pruned TFLite models\n","Create a compressible model for TFLite"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmpji7epnik/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmpji7epnik/assets\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stderr","output_type":"stream","text":["2022-11-06 18:19:19.372287: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n","2022-11-06 18:19:19.372313: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n","2022-11-06 18:19:19.372623: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmpji7epnik\n","2022-11-06 18:19:19.374829: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n","2022-11-06 18:19:19.374850: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmpji7epnik\n","2022-11-06 18:19:19.383462: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n","2022-11-06 18:19:19.416693: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmpji7epnik\n","2022-11-06 18:19:19.432030: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 59434 microseconds.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmp25sd2wzo/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmp25sd2wzo/assets\n","2022-11-06 18:19:22.779513: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n","2022-11-06 18:19:22.779531: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n","2022-11-06 18:19:22.779650: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmp25sd2wzo\n","2022-11-06 18:19:22.781583: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n","2022-11-06 18:19:22.781621: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmp25sd2wzo\n","2022-11-06 18:19:22.789143: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n","2022-11-06 18:19:22.819571: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/48/lcqkmdxs143dt92dvrnfr3nh0000gn/T/tmp25sd2wzo\n","2022-11-06 18:19:22.831735: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 52084 microseconds.\n"]}],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","pruned_tflite_model = converter.convert()\n","\n","with open(PRUNED_MODEL_PATH + '/OMV_Pruned_cnn.tflite', 'wb') as f:\n","    f.write(pruned_tflite_model)\n","\n","model_for_export2 = tfmot.sparsity.keras.strip_pruning(frozen_pruned_model)\n","model_for_export2.save(PRUNED_MODEL_PATH + '/OMV_Frozen_pruned_cnn.h5', include_optimizer=False)\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export2)\n","pruned_tflite_fmodel = converter.convert()\n","\n","with open(PRUNED_MODEL_PATH + '/OMV_Pruned_cnn.tflite', 'wb') as f:\n","    f.write(pruned_tflite_fmodel)"]},{"cell_type":"markdown","metadata":{},"source":["Compare size of the models"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'FROZEN_MODEL_PATH' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSize of gzipped baseline Keras model: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (get_gzipped_model_size(FROZEN_MODEL_PATH \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmnist_cnn.h5\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSize of gzipped pruned Keras model  : \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (get_gzipped_model_size(PRUNED_MODEL_PATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/OMV_Pruned_cnn.h5\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSize of gzipped pruned TFlite model : \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (get_gzipped_model_size(PRUNED_MODEL_PATH \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/OMV_Pruned_cnn.tflite\u001b[39m\u001b[39m'\u001b[39m)))\n","\u001b[0;31mNameError\u001b[0m: name 'FROZEN_MODEL_PATH' is not defined"]}],"source":["print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(FROZEN_MODEL_PATH + \"mnist_cnn.h5\")))\n","print(\"Size of gzipped pruned Keras model  : %.2f bytes\" % (get_gzipped_model_size(PRUNED_MODEL_PATH + '/OMV_Pruned_cnn.h5')))\n","print(\"Size of gzipped pruned TFlite model : %.2f bytes\" % (get_gzipped_model_size(PRUNED_MODEL_PATH + '/OMV_Pruned_cnn.tflite')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save featyres for pruned model\n","\n","features_pruned = frozen_pruned_model.predict(data_test, verbose = False)\n","\n","np.savetxt(FROZEN_PRUNED_MODEL_PATH +'/'+'ll_features_p_10.txt',features_pruned, fmt='%.3f')\n","with open(FROZEN_PRUNED_MODEL_PATH +'/'+'ll_features_p_10.txt', 'w') as new_file:\n","\n","    for i in range(0, features_pruned.shape[0]):\n","        for j in range(0, features_pruned.shape[1]): \n","            if features_pruned[i,j] != 0:\n","              str1 = '%.3f '%features_pruned[i,j]\n","            else:\n","              str1 = '0 '\n","\n","            new_file.write(str1)\n","            \n","        new_file.write('\\n')     \n","\n","new_file.close()\n","\n","np.savetxt(FROZEN_PRUNED_MODEL_PATH +'/'+'ll_labels_features_p_10.txt',label_test, fmt='%1d')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"}}},"nbformat":4,"nbformat_minor":0}
