

def printEvalMetrics(model, x_train, x_test, y_train, y_test):

    # this function uses both train and test dataset to evaluate the OL method
    standard_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] # order of labels that is used in all plots
    print("\n*** Printing evaluation metrics ***")
    cntr = 1
    learn_rate = model.l_rate
    train_samples = x_train.shape[0]
    test_samples = x_test.shape[0]
    tot_samples = train_samples + test_samples

    # PRINT OVERALL TRAINING PERFORMANCE METRICS
    pred_label = []
    # Cycle over all samples
    for i in range(0, tot_samples):
        if i < train_samples:
            current_label = y_train[i]
        else:
            current_label = y_test[i - train_samples]

        CheckLabelKnown(model, current_label)
        y_true_soft = DigitToSoftmax(current_label, model.label)

        # PREDICTION
        if (i < train_samples):
            y_ML = model.ML_frozen.predict(x_train[i, :].reshape(1, x_train.shape[1]))

            if (DEBUG_HISTORY == 1):
                temp = np.copy(np.array(np.matmul(y_ML, model.W) + model.b))
                temp = temp[0]
        else:
            y_ML = model.ML_frozen.predict(x_test[i - train_samples, :].reshape(1, x_test.shape[1]))
        y_pred = model.predict(y_ML[0, :])
        pred_label.append(y_pred)

    #pred_label = model.predict(x_test)
    y_all = np.concatenate((y_train, y_test))
    # true_label = myTest.letterToSoft_all(y_all, standard_label)
    true_label = DigitToSoftmax(y_all, standard_label)
    pred_label = np.argmax(pred_label, axis=1)
    true_label = np.argmax(true_label, axis=1)

    # importing confusion matrix
    from sklearn.metrics import confusion_matrix

    confusion = confusion_matrix(true_label, pred_label)
    print('Confusion Matrix\n')
    print(confusion)

    # importing accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

    print('\nAccuracy: {:.2f}\n'.format(accuracy_score(true_label, pred_label)))

    print('Micro Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='micro')))
    print('Micro Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='micro')))
    print('Micro F1-score: {:.2f}\n'.format(f1_score(true_label, pred_label, average='micro')))

    print('Macro Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='macro')))
    print('Macro Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='macro')))
    print('Macro F1-score: {:.2f}\n'.format(f1_score(true_label, pred_label, average='macro')))

    print('Weighted Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='weighted')))
    print('Weighted Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='weighted')))
    print('Weighted F1-score: {:.2f}'.format(f1_score(true_label, pred_label, average='weighted')))

    from sklearn.metrics import classification_report

    print('\nClassification Report\n')
    print(classification_report(true_label, pred_label, target_names=['A', 'E', 'I', 'O', 'U']))
