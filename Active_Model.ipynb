{"cells":[{"cell_type":"markdown","metadata":{"id":"D8g3_OkUNOuD"},"source":["## **Import the TensorFlow library**"]},{"cell_type":"markdown","metadata":{"id":"UKk-D3IZkkbE"},"source":["This code contains a small script that is used for generating a folder full of images. The idea is to show these images to the OpenMV camera when the training with the OL methos is required. Since the idea is to apply a supervised training is required to have a ground truth forthe computation of the error that the model is doing. This script fills a directory with images of digits and creates a txt file in which all the labels are saved. "]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":7796,"status":"ok","timestamp":1663535019276,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"XCqcQuaBLNgF"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Dropout, Dense, Flatten, Reshape\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import optimizers\n","\n","import matplotlib.pyplot as plt \n","import matplotlib.image as mpimg\n","from PIL import Image\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import random \n","import csv \n","import sys\n","import os\n","import re\n","from random import seed\n","\n","import time\n","#import os, os.path\n","import cv2\n","import glob\n","from keras import applications\n","#from keras.applications import vgg19\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","from numpy.ma.core import size\n","\n","import torchvision.models as models\n","from torchvision import transforms\n","ROOT_PATH = os.path.abspath('')\n","sys.path.insert(0, ROOT_PATH + '/lib')"]},{"cell_type":"markdown","metadata":{},"source":["# VS Code"]},{"cell_type":"markdown","metadata":{},"source":["# IMPORT AND LOAD MODELS"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-10-20 09:14:32.216274: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-10-20 09:14:40.247912: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from sklearn.cluster import KMeans\n","\n","\n","import random \n","from random import seed\n","\n","import time\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# Load features and labels\n","features_saved = np.loadtxt('Models/Original_model/ll_features_10.txt')\n","labels_features_saved = np.loadtxt('Models/Original_model/ll_labels_features_10.txt').astype(int)\n","\n","# Load model - CFR\n","model = keras.models.load_model('Models/Original_model/mnist_cnn.h5') # Frozen model \n","\n","# Absolute path is needed to load libraries \n","ROOT_PATH = os.path.abspath('')\n","sys.path.append(ROOT_PATH + '/lib')\n","\n","\n","\n","# Test import\n","# from lib import simulation_lib\n","# from lib.simulation_lib import\n","\n","from lib import Kmeans_lib\n","from lib.Kmeans_lib import *\n","from lib.EvalMetrics import *\n"]},{"cell_type":"markdown","metadata":{"id":"xT-FwynaWeYP"},"source":["# LOAD DATASETS"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1663535019950,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"R0r5dpq1WeYQ","outputId":"2d5a59cc-905b-4225-d757-0901d8a727d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["The original dataset shapes from MNIST are\n","    Train dataset shape: (60000, 28, 28)\n","    Test dataset shape:  (10000, 28, 28)\n"]}],"source":["(data_train, label_train),(data_test, label_test) = mnist.load_data() # Load data\n","print('The original dataset shapes from MNIST are')\n","print(f'    Train dataset shape: {data_train.shape}')\n","print(f'    Test dataset shape:  {data_test.shape}')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1663535020350,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"8eEeuvvEWeYR"},"outputs":[{"name":"stdout","output_type":"stream","text":["After the separation of the dataset in groups higer and lower/equal than 6 the datasets are:\n","     Train dataset lower than 6 has shape:  (36017, 28, 28, 1)\n","     Train dataset higher than 6 has shape: (23983, 28, 28, 1)\n","\n","     Test dataset lower than 6 has shape:  (6031, 28, 28, 1)\n","     Test dataset higher than 6 has shape: (3969, 28, 28, 1)\n"]}],"source":["train_samples = label_train.shape[0]\n","test_samples  = label_test.shape[0]\n","img_rows, img_cols = 28, 28\n","\n","trainLow_samples = np.sum(np.where(label_train < 6, 1, 0))\n","testLow_samples  = np.sum(np.where(label_test  < 6, 1, 0))\n","\n","# separate in containers data that is lower and higer than 6\n","# TRAIN - LOW\n","data_low_train   = np.zeros([trainLow_samples,28,28])\n","label_low_train  = np.zeros(trainLow_samples)\n","#       - HIGH\n","data_high_train  = np.zeros([train_samples-trainLow_samples,28,28])\n","label_high_train = np.zeros(train_samples-trainLow_samples)\n","\n","# TEST - LOW\n","data_low_test   = np.zeros([testLow_samples,28,28])\n","label_low_test  = np.zeros(testLow_samples)\n","\n","#      - HIGH\n","data_high_test  = np.zeros([test_samples-testLow_samples,28,28])\n","label_high_test = np.zeros(test_samples-testLow_samples)\n","\n","j,k = 0,0\n","for i in range(0,train_samples):  \n","    if(label_train[i]<6):\n","        data_low_train[j,:,:] = data_train[i,:,:]\n","        label_low_train[j]    = label_train[i]\n","        j+=1\n","    else:\n","        data_high_train[k,:,:] = data_train[i,:,:]\n","        label_high_train[k]    = label_train[i]\n","        k+=1\n","\n","\n","j,k = 0,0\n","for i in range(0,test_samples):\n","    if(label_test[i]>5):\n","        data_high_test[k,:,:] = data_test[i,:,:]\n","        label_high_test[k]    = label_test[i]\n","        k+=1  \n","    else:\n","        data_low_test[j,:,:] = data_test[i,:,:]\n","        label_low_test[j]    = label_test[i]\n","        j+=1\n","\n","# Reshape arrays\n","data_low_train  = data_low_train.reshape(data_low_train.shape[0], img_rows, img_cols, 1)\n","data_high_train = data_high_train.reshape(data_high_train.shape[0], img_rows, img_cols, 1)\n","data_low_test   = data_low_test.reshape(data_low_test.shape[0], img_rows, img_cols, 1)\n","data_high_test  = data_high_test.reshape(data_high_test.shape[0], img_rows, img_cols, 1)\n","input_shape = (img_rows, img_cols, 1)\n","    \n","print('After the separation of the dataset in groups higer and lower/equal than 6 the datasets are:')\n","print(f'     Train dataset lower than 6 has shape:  {data_low_train.shape}')\n","print(f'     Train dataset higher than 6 has shape: {data_high_train.shape}')\n","print()\n","print(f'     Test dataset lower than 6 has shape:  {data_low_test.shape}')\n","print(f'     Test dataset higher than 6 has shape: {data_high_test.shape}')\n","\n","# Normalize the colors from 0-255 to 0-1\n","data_low_train  = data_low_train.astype(np.float32) / 255.0\n","data_high_train = data_high_train.astype(np.float32) / 255.0\n","data_low_test   = data_low_test.astype(np.float32) / 255.0\n","data_high_test  = data_high_test.astype(np.float32) / 255.0"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset on which the model will be TRAINED has shape (100, 28, 28, 1)\n","The dataset on which the model will be TESTED has shape  (100, 28, 28, 1)\n"]}],"source":["n_samples = 100 # 400\n","digits_train = np.zeros((n_samples,28,28))\n","digits_test = np.zeros((n_samples,28,28))\n","label_digits_train = np.zeros(n_samples)\n","label_digits_test = np.zeros(n_samples)\n","\n","for i in range(0, n_samples):\n","  n = random.randint(0,len(data_train)-1)\n","  digits_train[i,:,:] = data_train[n,:,:]\n","  label_digits_train[i] = label_train[n]\n","  m = random.randint(0,len(data_test)-1)\n","  digits_test[i,:,:] = np.copy(data_test[m,:,:])\n","  label_digits_test[i] = label_test[m]\n","\n","\n","digits_train  = digits_train.reshape(digits_train.shape[0], img_rows, img_cols, 1).astype(np.float32) / 255.0\n","digits_test = digits_test.reshape(digits_test.shape[0], img_rows, img_cols, 1).astype(np.float32) / 255.0\n","input_shape = (img_rows, img_cols, 1)\n","\n","print(f'The dataset on which the model will be TRAINED has shape {digits_train.shape}') # da cambiare con il nuovo set\n","print(f'The dataset on which the model will be TESTED has shape  {digits_test.shape}') # da cambiare con il nuovo set"]},{"cell_type":"markdown","metadata":{"id":"zNrxFnGeWeYY"},"source":["# FUNCTIONS"]},{"cell_type":"markdown","metadata":{"id":"rjrOJ9rSWeYc"},"source":["# TRAIN THE MODEL ON THE DIGITS 6-9 (OL METHOD)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def run_frozen():\n","    pass\n","\n","\n","\n","\n","def ComputeEvalMetrics(true_label, pred_label, labels_list):\n","\n","    print(true_label.shape)\n","    print(pred_label.shape)\n","    confusion = confusion_matrix(true_label, pred_label, labels = labels_list)\n","    print('Confusion Matrix\\n')\n","    print(confusion)\n","\n","    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(true_label, pred_label)))\n","\n","    print('Micro Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='micro')))\n","    print('Micro Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='micro')))\n","    print('Micro F1-score: {:.2f}\\n'.format(f1_score(true_label, pred_label, average='micro')))\n","\n","    print('Macro Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='macro')))\n","    print('Macro Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='macro')))\n","    print('Macro F1-score: {:.2f}\\n'.format(f1_score(true_label, pred_label, average='macro')))\n","\n","    print('Weighted Precision: {:.2f}'.format(precision_score(true_label, pred_label, average='weighted')))\n","    print('Weighted Recall: {:.2f}'.format(recall_score(true_label, pred_label, average='weighted')))\n","    print('Weighted F1-score: {:.2f}'.format(f1_score(true_label, pred_label, average='weighted')))\n","\n","    print('\\nClassification Report\\n')\n","    print('TO DO')\n","    # print(classification_report(true_label, pred_label, target_names=['A', 'E', 'I', 'O', 'U']))"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1663535027195,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"qXndowOwWeYZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting batch:0/2\n","label: 7 pseudolabel: 9\n","label: 4 pseudolabel: 9\n","label: 7 pseudolabel: 9\n","label: 5 pseudolabel: 9\n","label: 9 pseudolabel: 4\n","label: 5 pseudolabel: 8\n","label: 5 pseudolabel: 9\n","label: 3 pseudolabel: 1\n","label: 4 pseudolabel: 9\n","label: 5 pseudolabel: 3\n","label: 6 pseudolabel: 8\n","label: 6 pseudolabel: 8\n","label: 2 pseudolabel: 1\n","label: 2 pseudolabel: 1\n","label: 6 pseudolabel: 8\n","label: 4 pseudolabel: 9\n","label: 0 pseudolabel: 8\n","label: 4 pseudolabel: 9\n","label: 2 pseudolabel: 8\n","label: 4 pseudolabel: 9\n","label: 0 pseudolabel: 8\n","label: 3 pseudolabel: 7\n","Features extraction took 0.078767 seconds and Kmean clustering took 0.945200 seconds , with 78.0 % accuracy (22 errors)\n","Starting batch:1/2\n","label: 4 pseudolabel: 9\n","label: 8 pseudolabel: 2\n","label: 1 pseudolabel: 9\n","label: 7 pseudolabel: 1\n","label: 3 pseudolabel: 8\n","label: 4 pseudolabel: 3\n","label: 3 pseudolabel: 5\n","label: 7 pseudolabel: 5\n","label: 6 pseudolabel: 3\n","label: 4 pseudolabel: 6\n","label: 9 pseudolabel: 6\n","label: 8 pseudolabel: 0\n","label: 3 pseudolabel: 9\n","label: 1 pseudolabel: 0\n","label: 8 pseudolabel: 6\n","label: 6 pseudolabel: 9\n","label: 5 pseudolabel: 6\n","label: 1 pseudolabel: 7\n","label: 7 pseudolabel: 2\n","label: 6 pseudolabel: 2\n","label: 4 pseudolabel: 9\n","label: 8 pseudolabel: 4\n","label: 4 pseudolabel: 6\n","label: 3 pseudolabel: 7\n","label: 5 pseudolabel: 2\n","label: 5 pseudolabel: 0\n","label: 9 pseudolabel: 2\n","label: 0 pseudolabel: 3\n","label: 5 pseudolabel: 0\n","label: 0 pseudolabel: 7\n","label: 2 pseudolabel: 9\n","label: 0 pseudolabel: 1\n","label: 1 pseudolabel: 4\n","label: 8 pseudolabel: 7\n","label: 0 pseudolabel: 1\n","label: 1 pseudolabel: 5\n","label: 9 pseudolabel: 5\n","label: 5 pseudolabel: 6\n","label: 3 pseudolabel: 2\n","label: 4 pseudolabel: 8\n","label: 5 pseudolabel: 8\n","label: 7 pseudolabel: 1\n","label: 8 pseudolabel: 2\n","label: 7 pseudolabel: 9\n","label: 6 pseudolabel: 4\n","label: 6 pseudolabel: 9\n","label: 3 pseudolabel: 7\n","label: 6 pseudolabel: 0\n","label: 8 pseudolabel: 9\n","label: 2 pseudolabel: 3\n","label: 7 pseudolabel: 9\n","label: 2 pseudolabel: 3\n","label: 9 pseudolabel: 2\n","label: 9 pseudolabel: 2\n","label: 6 pseudolabel: 2\n","label: 3 pseudolabel: 5\n","label: 8 pseudolabel: 9\n","label: 6 pseudolabel: 0\n","label: 6 pseudolabel: 2\n","label: 1 pseudolabel: 3\n","label: 4 pseudolabel: 5\n","label: 0 pseudolabel: 7\n","label: 4 pseudolabel: 3\n","label: 8 pseudolabel: 1\n","label: 2 pseudolabel: 9\n","label: 1 pseudolabel: 2\n","label: 1 pseudolabel: 4\n","label: 9 pseudolabel: 6\n","label: 1 pseudolabel: 5\n","label: 8 pseudolabel: 5\n","label: 4 pseudolabel: 5\n","label: 8 pseudolabel: 0\n","label: 6 pseudolabel: 7\n","label: 2 pseudolabel: 7\n","label: 3 pseudolabel: 8\n","label: 2 pseudolabel: 5\n","label: 1 pseudolabel: 8\n","label: 4 pseudolabel: 7\n","label: 3 pseudolabel: 6\n","label: 8 pseudolabel: 5\n","label: 8 pseudolabel: 9\n","label: 0 pseudolabel: 6\n","label: 1 pseudolabel: 6\n","label: 4 pseudolabel: 6\n","label: 1 pseudolabel: 3\n","label: 4 pseudolabel: 7\n","label: 0 pseudolabel: 8\n","label: 1 pseudolabel: 6\n","label: 5 pseudolabel: 2\n","label: 3 pseudolabel: 5\n","Features extraction took 0.075866 seconds and Kmean clustering took 1.507427 seconds , with 9.999999999999998 % accuracy (90 errors)\n","The error in clustering is: 56 %\n","Errors: 112 Samples:  200\n","**********************************\n"," Performing training with OL\n","\n"]}],"source":["# NEW CODE --- NEED TESTS\n","'''Function to compute kmean clustering on the new dataset and the saved features'''\n","def k_mean_clustering(features_run, labels_run, features_saved, labels_saved, n_cluster, batch_size):\n","  \n","  # Convert list to nparray\n","  #features = np.array(features_saved)\n","  # features = features.astype('float32')\n","  #labels = np.array(labels_saved)  \n","\n","  labels_init_list = list(range(0, n_cluster)) # TEMPORARY\n","\n","  # Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n","  features = np.concatenate((features_saved, features_run))\n","  labels = np.append(labels_saved, labels_run).astype(int)\n","\n","  # Repeat until clustering is correct\n","  max_iter = 20\n","  iter = 0\n","  while True:\n","    # KMean Clustering\n","    k_mean = create_k_mean(features, n_cluster)\n","\n","    # Find pseudolabels for each new image\n","    # Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n","    clusters_saved = list(k_mean.labels_[0:len(labels_saved)])\n","    labels_saved = list(labels_saved)\n","    cluster_list = list(range(0,n_cluster))\n","    map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_saved, labels_saved, cluster_list, labels_init_list) \n"," \n","    if len(map_clu2lbl) == n_cluster or iter > max_iter:\n","        if(iter > max_iter):\n","          print(\"Clustering did not converge. Skipping batch.NOT WORKING ---- NEED FIX-----\")\n","        # Exit the loop\n","        break\n","\n","  clusters_features = k_mean.labels_\n","\n","  # Find pseudo label (labels obtained from the model of each image\n","  pseudolabels = np.zeros(len(clusters_features), dtype=int)\n","\n","  # Compute pseudolabels only for new digits\n","  err = 0 # Initialize error counter\n","  for i in range(len(clusters_features) - batch_size, len(clusters_features)):\n","    pseudolabels[i] = map_clu2lbl[clusters_features[i]]\n","\n","    #print(pseudolabels[i], labels[i], clusters_features[i])\n","    if pseudolabels[i] != labels[i]:\n","      err += 1\n","      print(\"label:\",labels[i] ,\"pseudolabel:\", pseudolabels[i])\n","  \n","  # Evaluation metrics\n","  # ComputeEvalMetrics(labels_features, pseudolabels_features, labels_init_list)\n","\n","\n","  return pseudolabels, err\n","\n","''' Function to check if the current label is already known to the model. If not it augments the custom layer adding a new node'''\n","def CheckLabelKnown(model, current_label):\n","    \n","    found = False\n","    \n","    for i in range(0, len(model.label)):\n","        if(current_label == model.label[i]):\n","            found = True\n","          \n","    # If the label is not known\n","    if not found:\n","       #  print(f'\\n\\n    New digit detected -> digit \\033[1m{current_label}\\033[0m \\n')\n","        print(\"new digit detected\", current_label )\n","\n","        model.label.append(current_label)   # Add new digit to label\n","                \n","        # Increase weights and biases dimensions\n","        model.W = np.hstack((model.W, np.zeros([model.W.shape[0],1])))\n","        model.b = np.hstack((model.b, np.zeros([1])))\n","        \n","        model.W_2 = np.hstack((model.W_2, np.zeros([model.W.shape[0],1])))\n","        model.b_2 = np.hstack((model.b_2, np.zeros([1])))\n","\n","\n","def update_active_layer(model, features, pseudo_label):\n","\n","    learn_rate = model.l_rate\n","\n","    CheckLabelKnown(model, pseudo_label)\n","    \n","    y_true_soft = DigitToSoftmax(pseudo_label, model.label)\n","               \n","    # Prediction\n","    y_pred = model.predict(features)\n","        \n","    # Backpropagation\n","    cost = y_pred-y_true_soft\n","        \n","    for j in range(0,model.W.shape[0]):\n","\n","         # Update weights\n","        dW = np.multiply(cost, features[j]*learn_rate)\n","        model.W[j,:] = model.W[j,:]-dW\n","\n","    # Update biases\n","    db      = np.multiply(cost, learn_rate)\n","    model.b = model.b-db\n","           \n","    # the next part is only to plot the confusion matrix\n","    # if the train data is finished still train the model but do not save the results\n","    #if(i>=train_samples):\n","\n","    #    y_true_soft = DigitToSoftmax(y_tot[i], model.label)\n","                   \n","        # Find the max iter for both true label and prediction\n","    #    if(np.amax(y_true_soft) != 0):\n","    #        max_i_true = np.argmax(y_true_soft)\n","\n","    #    if(np.amax(y_pred) != 0):\n","    #        max_i_pred = np.argmax(y_pred)\n","\n","\n","\n","\n","\n","\n","def trainOneEpoch_OL(model, images, labels, features_saved, labels_saved, batch_size):\n","    \n","    n_cluster = 10\n","    n_samples = images.shape[0]\n","\n","    # Features extraction\n","    # features = model.ML_frozen.predict(images.reshape((1,28,28,1)), verbose = False)\n","\n","\n","\n","\n","    # Define initial set of features\n","    labels_init_list = list(range(0, n_cluster))\n","\n","    # labels_init_list = list([1, 9, 5, 0])\n","    # labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","    # n_cluster = len(labels_init_list)\n","\n","    # Extract from the saved features the labels that we need\n","    features_saved_sel = []\n","    labels_saved_sel = []\n","    # Extract features of digits considered in labels_init_list\n","    for i in range(0, len(features_saved)):\n","        if labels_features_saved[i] in labels_init_list:\n","            features_saved_sel.append(features_saved[i,:])\n","            labels_saved_sel.append(labels_saved[i])\n","\n","\n","    # Split dataset in batches\n","    n_batch = int(np.ceil(n_samples / batch_size))\n","    images_batch = np.array_split(images, n_batch)\n","    labels_batch = np.array_split(labels, n_batch)\n","\n","\n","    err_tot = 0\n","    pseudo_labels = np.empty\n","    for i in range(0, n_batch):\n","        print(\"Starting batch:\" + str(i) + \"/\" + str(n_batch))\n","        # Features extraction\n","        start1 = time.time()\n","        features = model.ML_frozen.predict(images_batch[i].reshape((batch_size,28,28,1)), verbose = False) # VERIFICO\n","        end1 = time.time()\n","\n","        # Kmean clustering\n","        start2 = time.time()\n","        pseudo_label, err = k_mean_clustering(features, labels_batch, features_saved_sel, labels_saved_sel, n_cluster, batch_size)\n","        end2 = time.time()\n","        pseudo_labels = np.append(pseudo_labels, pseudo_label.astype(int))\n","        err_tot += err\n","\n","        print(\"Features extraction took {:f} seconds\".format(end1 - start1), \"and Kmean clustering took {:f} seconds\".format(end2 - start2), \", with {}\".format((1-err/features.shape[0])*100), \"%\", \"accuracy\", \"({} errors)\".format(err))\n","\n","    print(\"The error in clustering is: {}\".format(int(err_tot/n_samples*100)), '%')\n","    print(\"Errors:\", err_tot, \"Samples: \", n_samples)\n","\n","    # ONLINE-LEARNING\n","\n","    print('**********************************\\n Performing training with OL\\n')\n","\n","    for i in range(0, n_samples):\n","        pass\n","        # update_active_layer(model, features[i,:], pseudo_labels[i])\n","\n","\n","\n","\n","\n","digits = np.concatenate((digits_train, digits_test))   \n","labels = np.concatenate((label_digits_train, label_digits_test))  \n","trainOneEpoch_OL(Model_OL, digits, labels, features_saved, labels_features_saved, batch_size)\n"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["\n","\n","# new testing\n","'''Function to compute kmean clustering on the new dataset and the saved features'''\n","def k_mean_clustering(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size):\n","\n","  # Define initial set of features\n","  labels_init_list = list(range(0, n_cluster))\n","\n","  # labels_init_list = list([1, 9, 5, 0])\n","  # labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","  # n_cluster = len(labels_init_list)\n","\n","  # Extract from the saved features the labels that we need\n","  features_saved_init = []\n","  labels_features_saved_init = []\n","  # Extract features of digits considered in labels_init_list\n","  for i in range(0, len(features_saved)):\n","      if labels_features_saved[i] in labels_init_list:\n","        features_saved_init.append(features_saved[i,:])\n","        labels_features_saved_init.append(labels_features_saved[i])\n","  \n","  # Convert list to nparray\n","  features = np.array(features_saved_init)\n","  features = features.astype('float32')\n","  labels_features = np.array(labels_features_saved_init)  \n","\n","  # Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n","  features = np.concatenate((features, features_run))\n","  labels_features = np.append(labels_features, labels_features_run).astype(int)\n","\n","  # Repeat until clustering is correct\n","  while True:\n","    # KMean Clustering\n","    k_mean = create_k_mean(features, n_cluster)\n","\n","    # Find pseudolabels for each new image\n","    # Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n","    clusters_features_saved = list(k_mean.labels_[0:len(labels_features_saved_init)])\n","    labels_features_saved_init = list(labels_features_saved_init)\n","    cluster_list = list(range(0,n_cluster))\n","    map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_features_saved, labels_features_saved_init, cluster_list, labels_init_list) \n"," \n","    if len(map_clu2lbl) == n_cluster:\n","        # Exit the loop\n","        break\n","\n","  clusters_features = k_mean.labels_\n","\n","  # Find pseudo label (labels obtained from the model of each image\n","  # pseudolabels = np.zeros(len(clusters_features), dtype=int)\n","\n","  #print(\"cluster features\", clusters_features)\n","  #print(\"labels features\", labels_features)\n","\n","\n","  pseudolabels = []\n","\n","  # Compute pseudolabels only for new digits\n","  err = 0 # Initialize error counter\n","  for i in range(len(clusters_features) - batch_size, len(clusters_features)):\n","    pseudolabel = map_clu2lbl[clusters_features[i]]\n","    pseudolabels.append(pseudolabel)\n","\n","\n","    # print(pseudolabels_features[i], labels_features[i], clusters_features[i])\n","    if pseudolabel != labels_features[i]:\n","      err += 1\n","  \n","  # print(pseudolabels_features)\n","  # print(labels_features[len(clusters_features)-batch_size:len(clusters_features)])\n","\n","  # conf_matrix(clusters_features_saved, labels_features_saved_init, , labels_init_list)\n","  \n","  # Evaluation metrics\n","  # ComputeEvalMetrics(labels_features, pseudolabels_features, cluster_list, labels_init_list)\n","\n","  return pseudolabels, err\n","\n","\n","########################################################################################################\n","########################################################################################################\n","def update_active_layer(model, features, pseudolabel):\n","\n","    learn_rate = model.l_rate\n","\n","    CheckLabelKnown(model, pseudolabel)\n","    \n","    y_true_soft = DigitToSoftmax(pseudolabel, model.label)\n","               \n","    # Prediction\n","    y_pred = model.predict(features)\n","        \n","    # Backpropagation\n","    cost = y_pred-y_true_soft\n","        \n","    for j in range(0,model.W.shape[0]):\n","\n","         # Update weights\n","        dW = np.multiply(cost, features[j]*learn_rate)\n","        model.W[j,:] = model.W[j,:]-dW\n","\n","    # Update biases\n","    db      = np.multiply(cost, learn_rate)\n","    model.b = model.b-db\n","\n","\n","\n","########################################################################################################\n","########################################################################################################\n","def trainOneEpoch_OL(model, x_tot, y_tot, features_saved, labels_features_saved, batch_size):\n","       \n","    learn_rate = model.l_rate\n","    n_cluster = 10\n","    n_samples = x_tot.shape[0]\n","\n","    # cntr_clus = 0\n","    # reminder = n_samples%batch_size\n","    # max_iter = int(n_samples//batch_size)\n","\n","    \n","    #print('**********************************\\n Performing clustering\\n')\n","\n","    # BATCH PROCESSING OF DATA\n","    n_batch = int(np.ceil(n_samples / batch_size))\n","    images_batch = np.array_split(x_tot, n_batch)\n","    labels_batch = np.array_split(y_tot, n_batch)\n","\n","    err_tot = 0\n","    pseudo_labels = []\n","    for i in range(0, n_batch):\n","        print(\"Starting batch:\" + str(i+1) + \"/\" + str(n_batch))\n","        # Features extraction\n","        features_batch = model.ML_frozen.predict(images_batch[i].reshape((batch_size,28,28,1)), verbose = False)\n","\n","        # Kmean clustering\n","        pseudo_labels_batch, err = k_mean_clustering(features_batch, features_saved, labels_batch[i], labels_features_saved, n_cluster, batch_size)\n","        # pseudo_labels.append(pseudo_label) \n","        pseudo_labels.extend(pseudo_labels_batch)\n","        err_tot += err\n","\n","        print(\"batch error\", err, \"total error\", err_tot)\n","        # print(\"Features extraction took {:f} seconds\".format(end1 - start1), \"and Kmean clustering took {:f} seconds\".format(end2 - start2), \", with {}\".format((1-err/features.shape[0])*100), \"%\", \"accuracy\", \"({} errors)\".format(err))\n","\n","    print(\"The error in clustering is: \", int(err_tot/n_samples*100), '%')\n","    print(\"errors:\", err_tot, \"tot samples\", n_samples)\n","\n","\n","\n","    # ONLINE-LEARNING\n","\n","    pseudo_labels = np.array(pseudo_labels)\n","    print(pseudo_labels.shape)\n","\n","\n","\n","    print('**********************************\\n Performing training with OL\\n')\n","\n","    features_images = model.ML_frozen.predict(x_tot.reshape((n_samples,28,28,1)), verbose = False)\n","    \n","\n","\n","    for i in range(0, n_samples):\n","        update_active_layer(model, features_images[i,:], pseudo_labels[i])\n","\n","\n","        # CheckLabelKnown(model, pseudo_labels[i])\n","    \n","        # y_true_soft = DigitToSoftmax(pseudo_labels[i], model.label)\n","               \n","        # # Prediction\n","        \n","        # y_pred = model.predict(features_images[i,:])\n","        \n","        # # Backpropagation\n","        # cost = y_pred-y_true_soft\n","        \n","        # for j in range(0,model.W.shape[0]):\n","\n","        #     # Update weights\n","        #     dW = np.multiply(cost, features_images[i,j]*learn_rate)\n","        #     model.W[j,:] = model.W[j,:]-dW\n","\n","        # # Update biases\n","        # db      = np.multiply(cost, learn_rate)\n","        # model.b = model.b-db\n","        \n","        # # the next part is only to plot the confusion matrix\n","        # # if the train data is finished still train the model but do not save the results\n","        # if(i>=train_samples):\n","\n","        #     y_true_soft = DigitToSoftmax(y_tot[i], model.label)\n","                   \n","        #     # Find the max iter for both true label and prediction\n","        #     if(np.amax(y_true_soft) != 0):\n","        #         max_i_true = np.argmax(y_true_soft)\n","\n","        #     if(np.amax(y_pred) != 0):\n","        #         max_i_pred = np.argmax(y_pred)"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5562,"status":"ok","timestamp":1663535061362,"user":{"displayName":"Giovanni Poletti","userId":"02119684520637616053"},"user_tz":-120},"id":"Iu-Jd60vWeYd","outputId":"782ccaa2-20f3-4fe1-c24d-ec981160f645","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting batch:1/2\n","batch error 20 total error 20\n","Starting batch:2/2\n","batch error 22 total error 42\n","The error in clustering is:  21 %\n","errors: 42 tot samples 200\n","(200,)\n","**********************************\n"," Performing training with OL\n","\n","(512,)\n"]}],"source":["Model_OL = Custom_Layer(model)\n","Model_OL.title      = 'OL'\n","Model_OL.filename   = 'OL'\n","Model_OL.l_rate     = 0.01\n","Model_OL.batch_size = 8\n","\n","batch_size = 100 # 20\n","\n","import importlib\n","importlib.reload(Kmeans_lib)\n","\n","#from IPython.core.interactiveshell import InteractiveShell\n","#InteractiveShell.ast_node_interactivity = \"all\"\n","\n","backup = True\n","\n","if backup:\n","    # trainOneEpoch_OL(Model_OL, digits_train, digits_test,  label_digits_train,  label_digits_test, features_saved, labels_features_saved, batch_size)\n","    \n","    x_train = digits_train\n","    x_test = digits_test\n","\n","    y_train = label_digits_train\n","    y_test = label_digits_test\n","\n","    train_samples = x_train.shape[0]\n","    test_samples = x_test.shape[0]\n","    tot_samples = train_samples + test_samples\n","    \n","    # Merge train and test arrays\n","    x_tot = np.concatenate((x_train, x_test))   # Images\n","    y_tot = np.concatenate((y_train, y_test))   # Labels    \n","    \n","    \n","    trainOneEpoch_OL(Model_OL, x_tot, y_tot, features_saved, labels_features_saved, batch_size)\n","else:\n","    # Merge train and test datasets\n","    digits = np.concatenate((digits_train, digits_test))   \n","    labels = np.concatenate((label_digits_train, label_digits_test))  \n","    trainOneEpoch_OL(Model_OL, digits, labels, features_saved, labels_features_saved, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.042, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.245, 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.007, 0.715, 0.   , 0.   , 0.   , 0.524, 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.777, 0.053, 0.9  , 0.   , 0.   ,\n","       0.   , 0.972, 0.   , 0.   , 0.   , 1.784, 0.   , 0.   , 1.024,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.714, 2.935, 0.   ,\n","       0.   , 0.988, 2.111, 0.   , 0.   , 1.147, 0.   , 0.   , 0.463,\n","       1.97 , 0.   , 2.253, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.163, 0.698, 2.776, 0.   , 0.   , 0.   , 0.   , 2.963, 0.   ,\n","       1.29 , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 2.057,\n","       0.   , 0.   , 0.   , 0.462, 0.017, 0.   , 0.   , 0.   , 0.   ,\n","       0.278, 0.   , 0.   , 0.   , 0.   , 0.   , 0.686, 1.092, 0.   ,\n","       2.389, 0.   , 0.   , 1.416, 0.   , 1.505, 0.   , 1.37 , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.048, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 1.019, 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.727, 2.74 , 0.   , 0.   , 0.739, 1.092,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.481, 0.   , 1.745,\n","       0.   , 0.   , 0.   , 0.311, 0.   , 0.   , 0.   , 1.991, 0.   ,\n","       0.   , 0.243, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       2.214, 0.   , 0.   , 1.036, 0.   , 0.166, 0.   , 0.59 , 0.   ,\n","       0.   , 0.482, 0.877, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 1.144, 0.   , 0.   , 0.   , 0.   ,\n","       3.144, 0.   , 1.817, 0.   , 1.01 , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 2.666, 0.   , 0.   , 0.   , 0.447, 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.932,\n","       0.   , 0.   , 1.831, 0.   , 0.   , 0.   , 0.   , 1.214, 0.232,\n","       1.671, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.073, 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 1.034, 0.   , 0.   , 1.073, 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.664, 2.941, 0.   , 0.   ,\n","       0.943, 0.789, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.549,\n","       0.   , 1.698, 0.   , 0.   , 0.   , 0.93 , 0.   , 0.   , 0.   ,\n","       1.949, 0.707, 0.   , 0.   , 0.   , 0.   , 0.261, 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 2.292, 0.   ,\n","       1.307, 0.   , 0.   , 1.433, 0.   , 0.   , 0.   , 0.   , 0.064,\n","       0.972, 0.   , 0.   , 0.   , 0.   , 0.   , 1.703, 0.   , 0.813,\n","       0.   , 0.   , 3.684, 0.   , 2.292, 0.   , 1.895, 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 2.272, 0.   , 0.   , 0.   , 0.707, 0.   ,\n","       0.   , 0.031, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.91 , 0.   , 0.   , 1.289, 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.33 , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.783, 0.   , 0.   ,\n","       0.046, 0.   , 1.011, 0.   , 0.   , 0.   , 2.206, 0.   , 0.   ,\n","       1.575, 0.   , 0.735, 0.   , 0.   , 0.   , 0.598, 0.888, 2.167,\n","       0.   , 0.   , 1.59 , 1.235, 0.   , 0.   , 1.743, 0.   , 0.   ,\n","       0.   , 1.961, 0.   , 2.571, 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.06 , 0.   , 0.907, 0.336, 0.   , 0.   , 0.   , 0.   , 1.37 ,\n","       0.8  , 0.   , 1.153, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       2.413, 0.   , 1.259, 0.   , 0.   , 1.23 , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.666, 0.   , 0.   , 0.   , 0.   , 0.564, 0.   ,\n","       0.   , 2.   , 0.   , 0.   , 2.938, 0.   , 2.145, 0.469, 1.611,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 1.055, 0.   , 0.   , 0.   ,\n","       0.518, 0.   , 0.   , 0.037, 0.   , 0.   , 0.   , 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.827, 0.   , 0.   , 0.808, 0.   , 0.   ,\n","       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["feat = features_saved[1,:]\n","\n","##(features_saved.all)\n","#max(features_saved)\n","\n","feat.dtype\n","\n","np.amin(feat)\n","np.amax(feat)\n","\n","feat.shape\n","feat"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.        , 0.96312505, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.3460795 , 1.0170912 ,\n","        0.8546268 , 0.        , 0.        , 0.        , 0.79275155,\n","        0.57086986, 0.        , 0.93478656, 3.351025  , 0.        ,\n","        0.        , 1.7612993 , 0.        , 1.2195386 , 0.        ,\n","        0.        , 0.        , 0.5815387 , 0.64130604, 2.1060772 ,\n","        0.        , 0.        , 0.        , 2.6718721 , 0.        ,\n","        0.        , 4.112043  , 0.        , 0.        , 2.0145454 ,\n","        1.4062762 , 0.        , 4.5407233 , 0.        , 0.        ,\n","        0.72269887, 0.        , 0.        , 0.        , 0.        ,\n","        0.60950524, 3.2392838 , 0.        , 0.        , 1.0193931 ,\n","        2.779964  , 3.569485  , 0.        , 0.2462171 , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 1.8287938 ,\n","        1.0599822 , 1.351576  , 0.        , 0.21574514, 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 3.1663988 , 0.        , 1.1370498 , 2.7143831 ,\n","        2.7085648 , 2.972627  , 0.        , 1.3165327 , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.3480746 , 0.        , 0.        , 2.0746028 ,\n","        0.        , 0.        , 0.        , 1.5493635 , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.22989203, 4.16709   , 0.        , 0.        ,\n","        1.0403309 , 0.        , 0.        , 0.        , 0.85826725,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.00640748, 0.        , 0.9615716 , 0.        , 0.00625021,\n","        0.        , 0.        , 0.        , 1.1965523 , 0.        ,\n","        0.8440064 , 1.56524   , 0.        , 0.7740532 , 1.0802475 ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.2481153 , 0.19965503, 0.26728785, 0.        , 0.        ,\n","        0.6763042 , 0.        , 0.        , 0.        , 2.8527617 ,\n","        0.        , 0.        , 0.88118976, 1.2277213 , 0.        ,\n","        0.        , 0.        , 0.        , 1.5845977 , 1.1714157 ,\n","        0.        , 1.0395052 , 0.01915106, 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 3.2137814 ,\n","        0.7458649 , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.19499303, 0.        , 0.96235377,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.78980225,\n","        1.1520149 , 0.36482984, 0.        , 1.3893648 , 1.6613669 ,\n","        0.        , 2.3661392 , 0.        , 0.        , 0.        ,\n","        0.9818712 , 1.3542681 , 0.        , 0.79325914, 1.9764605 ,\n","        0.        , 0.        , 0.38440698, 0.        , 0.        ,\n","        0.        , 2.315907  , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.9016035 , 0.        , 0.        ,\n","        2.3263    , 0.        , 0.        , 1.3057499 , 0.        ,\n","        0.5323998 , 0.        , 2.747977  , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.5422049 , 0.14479767, 0.        ,\n","        0.        , 0.        , 0.8261775 , 0.        , 0.        ,\n","        0.        , 0.45876646, 0.4839968 , 0.        , 0.        ,\n","        0.        , 0.        , 0.10856057, 0.        , 0.5482737 ,\n","        1.8386887 , 0.        , 0.        , 1.427943  , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 2.0495045 , 0.        , 0.07352149, 0.35058737,\n","        0.        , 0.        , 1.7128376 , 0.        , 1.7231061 ,\n","        0.9565488 , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 1.4465069 , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.19465101, 0.        , 0.821671  , 0.        , 1.339329  ,\n","        0.        , 0.        , 1.6784303 , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 1.4980394 , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 2.249438  ,\n","        0.        , 0.        , 0.94379354, 0.        , 2.605264  ,\n","        0.        , 1.5358865 , 0.        , 0.49532554, 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 2.591253  ,\n","        0.        , 0.        , 0.        , 0.84086585, 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.93198115, 0.        , 0.        , 2.081336  , 0.        ,\n","        0.        , 0.        , 0.        , 1.0264001 , 0.        ,\n","        1.387199  , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.93610597, 0.8844457 ,\n","        0.        , 0.        , 0.        , 2.1320024 , 0.        ,\n","        0.02186792, 0.        , 2.6014707 , 0.        , 0.        ,\n","        2.139914  , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.8973687 , 1.0694996 , 2.7851262 , 0.        ,\n","        0.        , 1.5555499 , 0.68047667, 0.        , 0.        ,\n","        3.4908915 , 0.        , 0.        , 1.4276764 , 1.2962995 ,\n","        0.        , 2.8173623 , 0.        , 0.        , 2.25674   ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.9177578 , 0.        , 0.        , 0.        , 0.05567408,\n","        1.2281537 , 1.7475723 , 0.        , 0.44354087, 0.        ,\n","        0.        , 0.        , 1.6378758 , 0.        , 0.        ,\n","        0.06718027, 0.        , 2.0653586 , 0.        , 0.        ,\n","        0.7381193 , 0.        , 0.1835031 , 0.        , 0.        ,\n","        0.        , 1.76828   , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 1.4557345 , 0.281501  , 0.58934087,\n","        0.24809782, 1.2152005 , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 1.5554223 , 0.        , 0.        ,\n","        0.        , 0.1695054 , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.666165  , 0.        ,\n","        0.        , 2.7000363 , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.17445867, 0.9797008 , 0.        ,\n","        0.        , 0.        ]], dtype=float32)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X = digits_train[1,:]\n","YY = Model_OL.ML_frozen.predict(X.reshape((1,28,28,1)), verbose = False)\n","\n","# YY2 = np.array(YY as features_saved)\n","\n","np.amin(YY)\n","np.amax(YY)\n","\n","YY.shape\n","\n","np.set_printoptions(suppress=True)\n","\n","YY"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","#from simulation_lib import *\n","\n","#plot_barChart(Model_OL)\n","#plot_confMatrix(Model_OL)\n","#plot_table(Model_OL)\n","\n","# from lib import simulation_lib as sim_lib\n","# sim_lib.plot_barChart(Model_OL)\n","# sim_lib.plot_confMatrix(Model_OL)\n","# sim_lib.plot_table(Model_OL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","############\n","\n","# Dummy  main for debug\n","if False:\n","  n = range(0, 10)\n","  images = data_train[n,:,:]\n","  labels_features_run = label_train[n]\n","\n","  features_run = []\n","  for img in images:\n","    features_run.append(Custom_Layer(model).ML_frozen.predict(img.reshape((1,28,28,1)))/1000)\n","\n","  features_run = np.array(features_run).squeeze(axis=1)\n","\n","  n_cluster = 5 # ARGOMENTO NON USATO\n","  # AGGIORNARE PASSANDO LA LISTA DI LABELS DA CLUSTERIZZARE\n","  batch_size = 10\n","\n","  kmean = k_mean_clustering(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JNC3pztTWeYb"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"}}},"nbformat":4,"nbformat_minor":0}
