{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 14:17:41.589751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 14:17:47.963564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import random \n",
    "from random import seed\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load features and labels\n",
    "features_saved = np.loadtxt('Models/Original_model/ll_features_20.txt')\n",
    "labels_features_saved = np.loadtxt('Models/Original_model/ll_labels_features_20.txt').astype(int)\n",
    "\n",
    "\n",
    "# Load model - CFR\n",
    "model = keras.models.load_model('Models/Original_model/mnist_cnn.h5') # Frozen model \n",
    "\n",
    "# model_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n",
    "# model_frozen.compile()\n",
    "\n",
    "model_frozen = keras.models.load_model('Models/Frozen_model/omv_mnist_cnn.h5')\n",
    "\n",
    "# Print\n",
    "# model.summary()\n",
    "# model_frozen.summary()\n",
    "\n",
    "# Absolute path is needed to load libraries \n",
    "ROOT_PATH = os.path.abspath('')\n",
    "sys.path.append(ROOT_PATH + '/lib')\n",
    "\n",
    "\n",
    "\n",
    "# Test import\n",
    "# from lib import simulation_lib\n",
    "# from lib.simulation_lib import\n",
    "\n",
    "from lib import Kmeans_lib\n",
    "from lib.Kmeans_lib import *\n",
    "# from lib.EvalMetrics import *\n",
    "\n",
    "(data_train, label_train),(data_test, label_test) = mnist.load_data() # Load data\n",
    "\n",
    "train_samples = label_train.shape[0]\n",
    "test_samples  = label_test.shape[0]\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "trainLow_samples = np.sum(np.where(label_train < 6, 1, 0))\n",
    "testLow_samples  = np.sum(np.where(label_test  < 6, 1, 0))\n",
    "\n",
    "# separate in containers data that is lower and higer than 6\n",
    "# TRAIN - LOW\n",
    "data_low_train   = np.zeros([trainLow_samples,28,28])\n",
    "label_low_train  = np.zeros(trainLow_samples)\n",
    "#       - HIGH\n",
    "data_high_train  = np.zeros([train_samples-trainLow_samples,28,28])\n",
    "label_high_train = np.zeros(train_samples-trainLow_samples)\n",
    "\n",
    "# TEST - LOW\n",
    "data_low_test   = np.zeros([testLow_samples,28,28])\n",
    "label_low_test  = np.zeros(testLow_samples)\n",
    "\n",
    "#      - HIGH\n",
    "data_high_test  = np.zeros([test_samples-testLow_samples,28,28])\n",
    "label_high_test = np.zeros(test_samples-testLow_samples)\n",
    "\n",
    "j,k = 0,0\n",
    "for i in range(0,train_samples):  \n",
    "    if(label_train[i]<6):\n",
    "        data_low_train[j,:,:] = data_train[i,:,:]\n",
    "        label_low_train[j]    = label_train[i]\n",
    "        j+=1\n",
    "    else:\n",
    "        data_high_train[k,:,:] = data_train[i,:,:]\n",
    "        label_high_train[k]    = label_train[i]\n",
    "        k+=1\n",
    "\n",
    "\n",
    "j,k = 0,0\n",
    "for i in range(0,test_samples):\n",
    "    if(label_test[i]>5):\n",
    "        data_high_test[k,:,:] = data_test[i,:,:]\n",
    "        label_high_test[k]    = label_test[i]\n",
    "        k+=1  \n",
    "    else:\n",
    "        data_low_test[j,:,:] = data_test[i,:,:]\n",
    "        label_low_test[j]    = label_test[i]\n",
    "        j+=1\n",
    "\n",
    "# Reshape arrays\n",
    "data_low_train  = data_low_train.reshape(data_low_train.shape[0], img_rows, img_cols, 1)\n",
    "data_high_train = data_high_train.reshape(data_high_train.shape[0], img_rows, img_cols, 1)\n",
    "data_low_test   = data_low_test.reshape(data_low_test.shape[0], img_rows, img_cols, 1)\n",
    "data_high_test  = data_high_test.reshape(data_high_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Normalize the colors from 0-255 to 0-1\n",
    "data_low_train  = data_low_train.astype(np.float32) / 255.0\n",
    "data_high_train = data_high_train.astype(np.float32) / 255.0\n",
    "data_low_test   = data_low_test.astype(np.float32) / 255.0\n",
    "data_high_test  = data_high_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to compute kmean clustering on the new dataset and the saved features'''\n",
    "def k_mean_clustering(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size):\n",
    "\n",
    "  # Define initial set of features\n",
    "  # CURRENTLY SUPPORTS ONLY SEQUENTIAL SELECTION i.e. 0,1,2,3,4,5\n",
    "  \n",
    "  \n",
    "  # labels_init_list = list(range(0, n_cluster))\n",
    "\n",
    "  labels_init_list = list([1, 9, 5, 0])\n",
    "  labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "  n_cluster = len(labels_init_list)\n",
    "\n",
    "  # labels_init_list = list([1, 9, 5, 0])\n",
    "  # labels_init_list = list([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])\n",
    "  # n_clust_init = len(labels_init_list)\n",
    "\n",
    "  # Extract from the saved features the labels that we need\n",
    "  features_saved_init = []\n",
    "  labels_features_saved_init = []\n",
    "  # Extract features of digits considered in labels_init_list\n",
    "  for i in range(0, len(features_saved)):\n",
    "      if labels_features_saved[i] in labels_init_list:\n",
    "        features_saved_init.append(features_saved[i,:])\n",
    "        labels_features_saved_init.append(labels_features_saved[i])\n",
    "  \n",
    "  # Convert list to nparray\n",
    "  features = np.array(features_saved_init)\n",
    "  labels_features = np.array(labels_features_saved_init)  \n",
    "\n",
    "  # Concateno al vettore delle features iniziali le features della nuova batch da analizzare\n",
    "  # features = np.concatenate((features, features_run))\n",
    "  # labels_features = np.append(labels_features, labels_features_run).astype(int)\n",
    "\n",
    "  # KMean Clustering\n",
    "  k_mean = create_k_mean(features, n_cluster)\n",
    "\n",
    "  # Find pseudolabels for each new image\n",
    "  # Pseudolabels are computed by looking at the confusion matrix of the saved dataset (where ground truth is known)\n",
    "  clusters_features_saved = list(k_mean.labels_[0:len(labels_features_saved_init)])\n",
    "  labels_features_saved_init = list(labels_features_saved_init)\n",
    "  cluster_list = list(range(0,n_cluster))\n",
    "  map_clu2lbl, map_lbl2clu = cluster_to_label(clusters_features_saved, labels_features_saved_init, cluster_list, labels_init_list)\n",
    "\n",
    "  # print(map_clu2lbl)\n",
    "\n",
    "  print(set(labels_features))\n",
    "  print(set(labels_features_saved_init))\n",
    "\n",
    "  clusters_features = k_mean.labels_\n",
    "\n",
    "  # Find pseudo label (labels obtained from the model of each image\n",
    "  pseudolabels_features = np.zeros(len(clusters_features), dtype=int)\n",
    "  \n",
    "  for i in range(0, len(clusters_features)):\n",
    "    pseudolabels_features[i] = map_clu2lbl[clusters_features[i]]\n",
    "  \n",
    "\n",
    "  print(clusters_features)\n",
    "  print(pseudolabels_features)\n",
    "  print(labels_features)\n",
    "\n",
    "  print(len(clusters_features))\n",
    "\n",
    "  # labels_features = map_clu2lbl[]\n",
    "\n",
    "  # conf_matrix(clusters_features_saved, labels_features_saved_init, , labels_init_list)\n",
    "  \n",
    "  # Evaluation metrics\n",
    "  return k_mean\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[3 2 8 4 1 6 5 7 0 9]\n",
      "Img1 shape (512,)\n",
      "[2]\n",
      "-93.53816038717422\n"
     ]
    }
   ],
   "source": [
    "'''Function to compute kmean clustering on the new dataset and the saved features'''\n",
    "def k_mean_clustering2(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size):\n",
    "# def k_mean_clustering2(features_saved, labels_features_saved):\n",
    "\n",
    "  labels_init_list = list([1, 9, 5, 0])\n",
    "  labels_init_list = list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "  n_cluster = len(labels_init_list)\n",
    "\n",
    "  # Extract from the saved features the labels that we need\n",
    "  features_saved_init = []\n",
    "  labels_features_saved_init = []\n",
    "  # Extract features of digits considered in labels_init_list\n",
    "  for i in range(0, len(features_saved)):\n",
    "      if labels_features_saved[i] in labels_init_list:\n",
    "        features_saved_init.append(features_saved[i,:])\n",
    "        labels_features_saved_init.append(labels_features_saved[i])\n",
    "  \n",
    "  # Convert list to nparray\n",
    "  features = np.array(features_saved_init)\n",
    "  labels_features = np.array(labels_features_saved_init)  \n",
    "\n",
    "  kmeans = KMeans(n_cluster)\n",
    "\n",
    "  # Assegno i centroidi iniziali e calcolo le varianze dei cluster\n",
    "  # Initialize dictionary used to store images related to each cluster\n",
    "  # clu_images = dict.fromkeys(range(0, n_clust_init), [])\n",
    "  # creates dictionary using dictionary comprehension -> list [] is mutable object\n",
    "  features_saved_dict = { key : [] for key in labels_init_list}\n",
    "\n",
    "  for i in range(0, len(features_saved_init)):\n",
    "      lbl = labels_features_saved_init[i]\n",
    "      features_saved_dict[lbl].append(features_saved_init[i])\n",
    "\n",
    "  print(features_saved_dict[2][1][230])\n",
    "\n",
    "  # cluster_mean_dict = { key : [] for key in labels_init_list}\n",
    "  cluster_mean = []\n",
    "  # Converto list-of-arrays in 2D array\n",
    "  for key in labels_init_list:\n",
    "    features_saved_dict[key] = np.array(features_saved_dict[key])\n",
    "    cluster_mean.append(np.mean(features_saved_dict[key], axis=0))\n",
    "\n",
    "  \n",
    "  # Converto in 2D nparray\n",
    "  cluster_mean = np.array(cluster_mean)\n",
    "\n",
    "  # Bisogna fare un fit per evitare errori -> passo una immagine per ogni cluster\n",
    "  init_dataset = np.zeros([10, 512])\n",
    "\n",
    "  #init_dataset[0,:] = \n",
    "\n",
    "  kmeans.fit(cluster_mean)\n",
    "\n",
    "  print(kmeans.predict(cluster_mean))\n",
    "\n",
    "  img1 = features_saved_dict[1][1]\n",
    "\n",
    "\n",
    "  img1 = cluster_mean[1]\n",
    "\n",
    "\n",
    "  # img1 = img2\n",
    "\n",
    "  print(\"Img1 shape\", img1.shape)\n",
    "\n",
    "  # print(cluster_mean.shape)\n",
    "\n",
    "  img1 = features_saved_dict[1][1]\n",
    "\n",
    "  img1 = img1.reshape(1, -1)\n",
    "\n",
    "  print(kmeans.predict(img1))\n",
    "  print(kmeans.score(img1))\n",
    "  #print(kmeans.transform(img1).shape) \n",
    "\n",
    "  # kmeans.cluster_centers_ = np.array(cluster_mean)\n",
    "  # kmeans.predict(features_run)\n",
    "  #cd = kmeans.cluster_centers_\n",
    "  #kmeans.cluster_centers_ = \n",
    "  # print(cd.shape)\n",
    "\n",
    "\n",
    "  # Evaluation metrics\n",
    "\n",
    "  #print(features_saved_dict[1][1].shape)\n",
    "\n",
    "  #img1 = features_saved_dict[1][1]\n",
    "  #img1.reshape(1, -1)\n",
    "  #print(img1.shape)\n",
    "  #kmeans.predict(img1)\n",
    "\n",
    "\n",
    "  # FIT NEW DATA\n",
    "\n",
    "  return \n",
    "\n",
    "\n",
    "###########\n",
    "\n",
    "# k_mean_clustering2(features_saved, labels_features_saved)\n",
    "if True:\n",
    "  n = range(0, 10)\n",
    "  images = data_train[n,:,:]\n",
    "  labels_features_run = label_train[n]\n",
    "\n",
    "  features_run = []\n",
    "  for img in images:\n",
    "    features_run.append(Custom_Layer(model).ML_frozen.predict(img.reshape((1,28,28,1)), verbose = False)/1000)\n",
    "\n",
    "  features_run = np.array(features_run).squeeze(axis=1)\n",
    "\n",
    "  n_cluster = 5 # ARGOMENTO NON USATO\n",
    "  # AGGIORNARE PASSANDO LA LISTA DI LABELS DA CLUSTERIZZARE\n",
    "  batch_size = 10\n",
    "\n",
    "  kmean = k_mean_clustering2(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from tabnanny import verbose\n",
    "\n",
    "\n",
    "if True:\n",
    "  n = range(0, 10)\n",
    "  images = data_train[n,:,:]\n",
    "  labels_features_run = label_train[n]\n",
    "\n",
    "  features_run = []\n",
    "  for img in images:\n",
    "    features_run.append(Custom_Layer(model).ML_frozen.predict(img.reshape((1,28,28,1)), verbose = False)/1000)\n",
    "\n",
    "  features_run = np.array(features_run).squeeze(axis=1)\n",
    "\n",
    "  n_cluster = 5 # ARGOMENTO NON USATO\n",
    "  # AGGIORNARE PASSANDO LA LISTA DI LABELS DA CLUSTERIZZARE\n",
    "  batch_size = 10\n",
    "\n",
    "  kmean = k_mean_clustering2(features_run, features_saved, labels_features_run, labels_features_saved, n_cluster, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_label(clusters_features, labels_features, cluster_list, labels_init_list):\n",
    "\n",
    "  # 1: map labels to rows: e.g. [0, 5, 9, 1] -> [1, 3, 2, 0]\n",
    "  # VECCHIO CODICE, MIGLIORATO USANDO LE CLUSTER LABELS..\n",
    "  # cluster_label_idx = []\n",
    "  # for lbl in labels_features:\n",
    "  # cluster_label_idx.append(labels_features.index(lbl))\n",
    "\n",
    "  # 2: compute Confusion matrix (for the saved features)\n",
    "  cmtx = confusion_matrix(clusters_features, labels_features, cluster_list, labels_init_list)\n",
    "\n",
    "  # PREVIOUS SCRIPT USING CONFUUSION_MATRIX FROM SKLEARN\n",
    "  # from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "  # cmtx = confusion_matrix(clusters_features, labels_features)\n",
    "  # Remove empty rows and cols\n",
    "  # cmtx = cmtx[~np.all(cmtx == 0, axis=1)]\n",
    "  # cmtx = cmtx[:,~np.all(cmtx == 0, axis=0)]\n",
    "\n",
    "  # cmtxdisp = ConfusionMatrixDisplay(cmtx) # , display_labels =labels_init_list \n",
    "\n",
    "  # Map true-labels to clusters\n",
    "  # cmtx = np.array(cmtx)\n",
    "\n",
    "  print(cmtx)\n",
    "\n",
    "  print(\"labels saved\", labels_features, len(labels_features))\n",
    "  print(\"cllusters saved\", clusters_features, len(clusters_features))\n",
    "\n",
    "  map_idx = np.argmax(cmtx, axis = 1)  \n",
    "\n",
    "  # Find duplicates and fix..\n",
    "\n",
    "\n",
    "  print(map_idx)\n",
    "\n",
    "  # Fill dictionary with map\n",
    "  map_clu2lbl = {}\n",
    "  map_lbl2clu = {}\n",
    "  # labels_init_list_sorted = labels_init_list.sort()\n",
    "  labels_init_list.sort()\n",
    "  for i in range(0, len(map_idx)):\n",
    "    map_clu2lbl[map_idx[i]] = labels_init_list[i]\n",
    "    map_lbl2clu[labels_init_list[i]] = map_idx[i]\n",
    "  \n",
    "  # Mapping dictionary\n",
    "  # map_clu2lbl -> cluster: label\n",
    "  # map_lbl2clu -> label: cluster\n",
    "\n",
    "\n",
    "  print(map_lbl2clu)\n",
    "\n",
    "  return map_clu2lbl, map_lbl2clu\n",
    "\n",
    "\n",
    "'''Function to compute confusion matrix between cluster and labels'''\n",
    "def confusion_matrix(clusters_features_saved, labels_features_saved_init, cluster_list, labels_list):\n",
    "\n",
    "  cmtx = np.zeros([len(labels_list), len(cluster_list)])\n",
    "\n",
    "  for i in range(0, len(clusters_features_saved)):\n",
    "\n",
    "    cluster = clusters_features_saved[i]\n",
    "    label = labels_features_saved_init[i]\n",
    "\n",
    "    # Find indices\n",
    "    m = labels_list.index(label)\n",
    "    n = cluster_list.index(cluster)\n",
    "    cmtx[m,n] += 1\n",
    "  return cmtx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7fd1b55a667aa91d3f88049cb2b0330e965cb77ee086e9d0bbb787b7ff82ca0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
